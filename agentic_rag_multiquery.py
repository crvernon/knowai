import streamlit as st

# --- Page Config (MUST BE FIRST STREAMLIT COMMAND) ---
st.set_page_config(layout="wide")

import fitz  # PyMuPDF
import faiss
import os
import hashlib
import logging # Import logging
# Updated OpenAI/Azure imports
from langchain_openai import OpenAIEmbeddings, ChatOpenAI, AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS # Using community import
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field # For function calling output schema
# Import MultiQueryRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from dotenv import load_dotenv
from typing import List, TypedDict, Annotated, Sequence
import operator

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage

# --- Logging Setup ---
# Optional: Configure logging to see the queries generated by MultiQueryRetriever
logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)


# --- Azure OpenAI Configuration ---
# Load environment variables (especially Azure credentials)
load_dotenv()

api_key = os.getenv("AZURE_OPENAI_API_KEY", default=None)
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", default=None)
# Use a default deployment name if not set, or handle error if critical
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o") # Defaulting to gpt-4o deployment name
embeddings_deployment = os.getenv("AZURE_EMBEDDINGS_DEPLOYMENT", "text-embedding-3-large") # Defaulting embeddings deployment
openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01") # Defaulting API version

# --- Check for Azure Credentials (AFTER set_page_config) ---
azure_creds_valid = True
if not api_key or not azure_endpoint:
    st.error("Azure OpenAI API key or endpoint not found. Please set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT environment variables.", icon="üö®")
    azure_creds_valid = False
    # Consider stopping execution if credentials are required for core functionality
    # st.stop() # Uncomment if the app cannot proceed without credentials

# Constants for predefined questions
PREDEFINED_QUESTIONS = [
    "Enter my own question...",
    "Does the plan include vegetation management? If so, how much money is allocated to it?",
    "Does the plan include undergrounding? If so, how much money is allocated to it?",
    "Does the plan include PSPS? If so, how much money is allocated to it?",
    "How frequently does the utility perform asset inspections?",
    "Are there generation considerations, such as derating solar PV during smoky conditions?"
]

# --- Helper Functions (PDF Processing - Unchanged) ---

def extract_pages_from_pdf(pdf_file):
    """Extracts text page by page from an uploaded PDF file, returning a list of (page_number, text)."""
    pages_content = []
    try:
        pdf_bytes = pdf_file.getvalue()
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text()
            if text:
                pages_content.append((page_num + 1, text))
        doc.close()
        return pages_content
    except Exception as e:
        st.error(f"Error reading PDF: {e}", icon="üìÑ")
        return None

# --- Caching (Retriever Creation - Returns BASE retriever) ---
# NOTE: We now create the MultiQueryRetriever dynamically inside the graph node
@st.cache_resource(show_spinner="Processing PDF and building knowledge base...")
def get_base_retriever_from_pdf(file_hash): # Parameter name matches the variable passed
    """Processes PDF, creates Azure embeddings, builds FAISS store, returns BASE retriever."""
    if not api_key or not azure_endpoint:
        st.error("Azure credentials not available for PDF processing.", icon="üö®")
        return None

    # Use the passed file_hash to get the file from session state
    uploaded_file = st.session_state.get(f"file_{file_hash}", None)
    if not uploaded_file:
        st.error("Could not find file content in session state for processing.", icon="üîí")
        return None
    try:
        st.write(f"Processing file: {uploaded_file.name}")
        pages_data = extract_pages_from_pdf(uploaded_file)
        if not pages_data:
            st.warning("Could not extract text from the PDF.", icon="‚ö†Ô∏è")
            return None

        text_splitter = RecursiveCharacterTextSplitter(
            # Increased overlap slightly as suggested earlier
            chunk_size=1000, chunk_overlap=200, length_function=len
        )
        all_docs = []
        for page_num, page_text in pages_data:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                doc = Document(page_content=chunk, metadata={"page": page_num})
                all_docs.append(doc)

        if not all_docs:
            st.warning("Could not create any text chunks from the PDF.", icon="‚ö†Ô∏è")
            return None
        st.write(f"Split PDF into {len(all_docs)} chunks across {len(pages_data)} pages.")

        # --- Instantiate Azure Embeddings ---
        st.write(f"Using Azure embeddings deployment: {embeddings_deployment}")
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=embeddings_deployment,
            azure_endpoint=azure_endpoint,
            api_key=api_key,
            openai_api_version=openai_api_version
        )

        st.write("Building FAISS index with Azure embeddings...")
        vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)
        st.write("FAISS index built successfully.")
        # Return the base retriever, k value applied by MultiQueryRetriever later
        return vectorstore.as_retriever(search_kwargs={'k': 10}) # Base retriever fetches fewer docs per query
    except Exception as e:
        st.error(f"Error processing PDF and building vector store: {e}", icon="‚ùå")
        st.exception(e)
        return None

# --- LangGraph State Definition ---

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents (potentially filtered)
        original_documents: list of documents retrieved initially by MultiQueryRetriever
        base_retriever: The base FAISS retriever object (passed at initialization)
    """
    question: str
    generation: str
    documents: List[Document]
    original_documents: List[Document] # Store initially retrieved docs
    base_retriever: object # Store the BASE retriever

# --- LangGraph Nodes (Modified for MultiQuery Retrieval) ---

def retrieve_docs_multi_query(state: GraphState) -> GraphState:
    """
    Uses MultiQueryRetriever to retrieve documents based on variations of the question.
    Stores the combined, unique documents in 'original_documents'.

    Args:
        state (dict): The current graph state including 'base_retriever'.

    Returns:
        dict: Adds 'original_documents' key to state.
    """
    st.write(f"--- Retrieving Documents using Multi-Query ---")
    question = state["question"]
    base_retriever = state["base_retriever"]

    if not base_retriever:
        st.error("Base retriever not available.")
        return {"original_documents": [], "question": question}

    # Ensure Azure creds are valid before making LLM call for query generation
    if not azure_creds_valid:
         st.warning("Skipping multi-query generation due to missing Azure credentials.")
         # Fallback to simple retrieval? Or just return empty? Let's fallback.
         try:
             st.write("Falling back to simple retrieval...")
             documents = base_retriever.get_relevant_documents(question)
             st.write(f"Retrieved {len(documents)} documents via simple fallback.")
             return {"original_documents": documents, "question": question}
         except Exception as e:
             st.error(f"Error during simple fallback retrieval: {e}")
             return {"original_documents": [], "question": question}

    try:
        # --- Instantiate Azure LLM for Query Generation ---
        st.write(f"Using Azure chat deployment for query generation: {deployment}")
        llm_for_queries = AzureChatOpenAI(
            temperature=0, # Low temp for query generation
            api_key=api_key,
            openai_api_version=openai_api_version,
            azure_deployment=deployment,
            azure_endpoint=azure_endpoint,
        )

        # --- Create MultiQueryRetriever ---
        # It will use the llm_for_queries to generate different versions of the question
        # and the base_retriever to fetch documents for each version.
        # The number of queries generated defaults to 3, can be adjusted.
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever, llm=llm_for_queries
        )

        # --- Invoke the MultiQueryRetriever ---
        # This generates queries, retrieves docs for each, and de-duplicates.
        # The underlying base retriever uses k=10, so 3 queries might fetch up to 30 unique docs.
        # Adjust base retriever's k or number of generated queries if more docs are needed.
        unique_docs = multi_query_retriever.get_relevant_documents(query=question)

        st.write(f"Retrieved {len(unique_docs)} unique documents via Multi-Query.")
        # Store these initial documents separately
        return {"original_documents": unique_docs, "question": question}
    except Exception as e:
        st.error(f"Error during multi-query retrieval: {e}")
        st.exception(e)
        return {"original_documents": [], "question": question} # Return empty list on error

# --- Filter Documents Node (Unchanged Logic, Operates on 'original_documents') ---
class FilteredDocs(BaseModel):
    """Schema for the function call to identify indices of documents to keep."""
    keep_indices: List[int] = Field(description="List of zero-based indices of the documents that should be kept (i.e., are not structural or irrelevant).")

def filter_documents(state: GraphState) -> GraphState:
    """
    Filters the initially retrieved documents (from MultiQuery) to remove structurally irrelevant ones.

    Args:
        state (dict): The current graph state including 'original_documents'.

    Returns:
        dict: Updates 'documents' key in state with the filtered list.
    """
    st.write("--- Filtering Structurally Irrelevant Documents ---")
    original_documents = state["original_documents"] # Docs from MultiQuery
    question = state["question"]

    if not original_documents:
        st.write("No documents to filter.")
        return {"documents": []}

    if not azure_creds_valid:
         st.warning("Skipping filtering due to missing Azure credentials.")
         return {"documents": original_documents}

    try:
        st.write(f"Using Azure chat deployment for filtering: {deployment}")
        filtering_llm = AzureChatOpenAI(
            temperature=0,
            api_key=api_key,
            openai_api_version=openai_api_version,
            azure_deployment=deployment,
            azure_endpoint=azure_endpoint,
        ).bind_tools([FilteredDocs], tool_choice="FilteredDocs")

        doc_context = ""
        for i, doc in enumerate(original_documents):
            doc_context += f"--- Document Index {i} (Page {doc.metadata.get('page', 'N/A')}) ---\n{doc.page_content}\n\n"

        filtering_prompt = PromptTemplate(
            template="""You are an expert document analyst. Your task is to identify document chunks that are primarily structural elements like Table of Contents entries, page headers/footers, indices, or reference lists, which are unlikely to contain direct answers to content-based questions about a technical report.

            Analyze the following document chunks provided below, each marked with a 'Document Index'.

            Document Chunks:
            {doc_context}

            Based on the content of each chunk, identify the indices of the documents that ARE LIKELY TO CONTAIN SUBSTANTIVE CONTENT (prose, data, analysis, findings) and should be kept for further analysis. Ignore chunks that are just lists of section titles with page numbers (like a ToC), repetitive headers/footers, or bibliographies unless they uniquely contain relevant information not present elsewhere.

            Use the 'FilteredDocs' tool to return the list of indices to keep. Return ONLY the indices to keep.
            """,
            input_variables=["doc_context"],
        )

        filtering_chain = filtering_prompt | filtering_llm
        response = filtering_chain.invoke({"doc_context": doc_context})

        kept_indices = []
        if response.tool_calls and response.tool_calls[0]['name'] == 'FilteredDocs':
            kept_indices = response.tool_calls[0]['args'].get('keep_indices', [])
            st.write(f"LLM identified {len(kept_indices)} documents to keep out of {len(original_documents)}.")
        else:
             st.warning("Filtering LLM did not return expected format. Keeping all documents for safety.")
             return {"documents": original_documents}

        filtered_docs = [original_documents[i] for i in kept_indices if i < len(original_documents)]
        st.write(f"Filtered down to {len(filtered_docs)} documents.")
        return {"documents": filtered_docs}

    except Exception as e:
        st.error(f"Error during document filtering: {e}")
        st.exception(e)
        st.warning("Filtering failed. Passing all original documents to grading.")
        return {"documents": original_documents}


# --- Grade Documents Node (Unchanged Logic, Operates on filtered 'documents') ---
def grade_documents(state: GraphState) -> GraphState:
    """
    Determines whether the *filtered* documents are relevant to the question using Azure LLM.

    Args:
        state (dict): The current graph state including 'documents' (filtered list).

    Returns:
        dict: Updates 'documents' key based on relevance grade.
    """
    st.write("--- Grading Filtered Document Relevance (Azure LLM) ---")
    question = state["question"]
    documents = state["documents"] # Use the potentially filtered documents

    if not documents:
         st.write("No documents remaining after filtering to grade.")
         return {"documents": [], "question": question} # Skip grading if no docs

    if not azure_creds_valid:
         st.warning("Skipping grading due to missing Azure credentials.")
         return {"documents": documents, "question": question} # Assume relevant if cannot grade

    try:
        st.write(f"Using Azure chat deployment for grading: {deployment}")
        llm = AzureChatOpenAI(
            temperature=0,
            api_key=api_key,
            openai_api_version=openai_api_version,
            azure_deployment=deployment,
            azure_endpoint=azure_endpoint,
        )

        grading_function = {
            "name": "grade_relevance",
            "description": "Determine if the provided document chunks are relevant to the user's question.",
            "parameters": { "type": "object", "properties": { "relevant": { "type": "string", "enum": ["yes", "no"], "description": "Whether the documents contain information relevant to answering the question."}}, "required": ["relevant"]},
        }

        prompt = PromptTemplate(
            template="""You are a grader assessing relevance of potentially relevant document chunks to a user question. These chunks have already been pre-filtered to remove obvious structural elements.
            Analyze the following document chunks based on the user question:
            \n ------- \n
            User Question: {question}
            \n ------- \n
            Document Chunks:
            {documents}
            \n ------- \n
            Determine if these remaining chunks contain information that can directly answer the user's question. Respond using the 'grade_relevance' function call. Ensure your response strictly adheres to the function call format.
            """,
            input_variables=["question", "documents"],
        )

        doc_texts = "\n\n".join([d.page_content for d in documents])
        grader_chain = prompt | llm.bind_tools([grading_function], tool_choice="grade_relevance")
        response = grader_chain.invoke({"question": question, "documents": doc_texts})

        is_relevant = "no"
        if response.tool_calls and response.tool_calls[0]['name'] == 'grade_relevance':
            is_relevant = response.tool_calls[0]['args'].get('relevant', 'no')
            st.write(f"Relevance Grade: {is_relevant.upper()}")
        else:
             st.warning("Grader LLM did not return expected function call format.")
             is_relevant = "no"

        if is_relevant == "yes":
            st.write("Decision: Documents are relevant. Proceeding to generation.")
            return {"documents": documents, "question": question}
        else:
            st.write("Decision: Documents not relevant enough. Skipping generation.")
            return {"documents": [], "question": question}

    except Exception as e:
        st.error(f"Error during document grading: {e}")
        st.exception(e)
        return {"documents": [], "question": question} # Halt on error


# --- Generate Answer Node (Modified Prompt for Contextualization) ---
def generate_answer(state: GraphState) -> GraphState:
    """
    Generates an answer using the final list of relevant documents, using Azure LLM.
    Includes quotes (in quotation marks) and contextualizes them with surrounding information.

    Args:
        state (dict): The current graph state including 'documents' (filtered and graded list).

    Returns:
        dict: New key added to state, generation, that contains LLM generation.
    """
    st.write("--- Generating Answer (Azure LLM) ---")
    question = state["question"]
    documents = state["documents"] # These are the filtered and graded documents

    if not documents:
        st.warning("Generation called with no relevant documents.")
        return {"generation": None}

    if not azure_creds_valid:
         st.warning("Skipping generation due to missing Azure credentials.")
         return {"generation": "Could not generate answer due to missing credentials."}

    try:
        context = "\n\n".join([f"--- Context from Page {d.metadata.get('page', 'N/A')} ---\n{d.page_content}" for d in documents]) # Added separator for clarity

        prompt_template = """You are an expert assistant analyzing a technical report. Your task is to answer the user's question comprehensively based *only* on the provided context chunks, which have been filtered for relevance.

        Follow these instructions carefully:
        1.  Thoroughly read all provided context chunks, each marked with '--- Context from Page X ---'. Pay attention to the page numbers. Synthesize information found across multiple chunks if they relate to the same aspect of the question.
        2.  Answer the user's question directly based *only* on the information present in the context.
        3.  Identify the most relevant information or statement(s) within the context that directly address the question.
        4.  **Contextualized Quoting:** When presenting this key information, include a direct quote (enclosed in double quotation marks) of the most relevant sentence or phrase. **Crucially, also explain the surrounding context from the *same chunk* to clarify the quote's meaning or provide necessary background.** For example, instead of just `"quote..." (Page X)`, you might say: `The report discusses mitigation strategies, stating, "quote..." (Page X), which is part of a larger section detailing preventative measures.` Cite the page number in parentheses after the quote.
        5.  If the question asks about specific details (like financial allocations, frequencies, specific procedures) and that detail is *not* found in the context, explicitly state that the information is not provided in the context. Do not make assumptions or provide external knowledge.
        6.  **Structure for Clarity:** Structure your answer logically. Start with a direct summary answer if possible. Then, present the supporting details using the contextualized quoting method described above. Ensure the explanation connects the quote and its context clearly back to the user's original question. Conclude by addressing any parts of the question that couldn't be answered from the context. If no relevant information is found in the provided context to answer the question, state that clearly.

        Context from Document (Chunks separated by '--- Context from Page X ---'):
        {context}

        Question: {question}

        Detailed Answer with Contextualized Quotes and Citations:"""
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        st.write(f"Using Azure chat deployment for generation: {deployment}")
        llm = AzureChatOpenAI(
            temperature=0.1,
            api_key=api_key,
            openai_api_version=openai_api_version,
            azure_deployment=deployment,
            azure_endpoint=azure_endpoint,
            # Potentially increase max_tokens if answers need to be longer due to context
            # max_tokens=1000
        )

        rag_chain = prompt | llm | StrOutputParser()
        generation = rag_chain.invoke({"context": context, "question": question})
        st.write("Answer generated.")
        return {"generation": generation}

    except Exception as e:
        st.error(f"Error during answer generation: {e}")
        st.exception(e)
        return {"generation": f"An error occurred while generating the answer: {e}"}


# --- LangGraph Conditional Edge (Unchanged Logic) ---
def decide_to_generate(state: GraphState) -> str:
    """
    Determines whether to generate an answer or end the process based on document relevance.
    """
    st.write("--- Checking Relevance for Generation ---")
    documents = state["documents"]
    if not documents:
        st.write("Decision: No relevant documents found after grading. Ending.")
        return "end_no_relevance"
    else:
        st.write("Decision: Relevant documents found. Proceeding to generate.")
        return "generate"

# --- Build LangGraph ---

# Define the workflow graph
workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("retrieve", retrieve_docs_multi_query) # Use the new multi-query retrieval node
workflow.add_node("filter_documents", filter_documents)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate_answer) # Node with updated prompt

# Build graph (same structure, just different retrieval node)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "filter_documents")
workflow.add_edge("filter_documents", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "generate": "generate",
        "end_no_relevance": END
    }
)
workflow.add_edge("generate", END)

# Compile the graph only if Azure credentials are valid
langgraph_app = None
if azure_creds_valid:
    try:
        langgraph_app = workflow.compile()
        st.success("LangGraph compiled successfully.")
    except Exception as e:
        st.error(f"Failed to compile LangGraph: {e}")
        st.exception(e)


# --- Streamlit UI ---

st.title("üìÑ Agentic AI Workflow (LangGraph - FAISS - MultiQuery)")
st.markdown("""
Upload a technical report (PDF) and ask questions about its content.
This version uses **Azure OpenAI** via LangGraph, **Multi-Query Retrieval**, filtering, grading, and generation.
Answers include direct quotes (in quotation marks) contextualized with surrounding details and page number citations.
""") # Updated markdown description

# --- File Upload ---
if azure_creds_valid:
    uploaded_file = st.file_uploader("Choose a PDF file (max 8MB recommended)", type="pdf")
else:
    uploaded_file = None
    st.warning("File uploader disabled until Azure credentials are provided.")


# --- Session State Initialization ---
if "file_hashes" not in st.session_state:
    st.session_state.file_hashes = {}

base_retriever = None # Renamed variable for clarity
# Only attempt to get retriever if Azure creds are valid and file is uploaded
if azure_creds_valid and uploaded_file is not None:
    file_content = uploaded_file.getvalue()
    # Calculate the hash using the correct variable name 'file_hash'
    file_hash = hashlib.md5(file_content).hexdigest()

    if file_hash not in st.session_state.file_hashes:
        st.session_state.file_hashes[file_hash] = uploaded_file.name
        # Use the correct variable name 'file_hash' for session state key
        st.session_state[f"file_{file_hash}"] = uploaded_file

    # Get the BASE retriever (MultiQueryRetriever is created dynamically)
    # *** FIX: Pass the correct variable name 'file_hash' ***
    base_retriever = get_base_retriever_from_pdf(file_hash)


# Clear session state if no file is present
if uploaded_file is None:
    st.session_state.file_hashes = {}
    keys_to_delete = [key for key in st.session_state if key.startswith("file_")]
    for key in keys_to_delete:
        del st.session_state[key]


# --- Question Input ---
st.divider()
st.subheader("Ask a Question")

col1, col2 = st.columns([3, 1])

with col1:
    selected_question = st.selectbox(
        "Select a predefined question or choose 'Enter my own question...':",
        PREDEFINED_QUESTIONS,
        index=0,
        key="question_select",
        disabled=not azure_creds_valid
    )
    custom_question_disabled = selected_question != PREDEFINED_QUESTIONS[0] or not azure_creds_valid
    custom_question = st.text_input(
        "Enter your custom question here:",
        key="custom_question_input",
        disabled=custom_question_disabled,
        placeholder="Type your question if you selected the first option above..." if azure_creds_valid else "Disabled due to missing credentials"
    )

with col2:
    st.markdown("<br/>", unsafe_allow_html=True)
    # Disable button if base_retriever or compiled graph is not available, or if creds are missing
    ask_button_disabled = not base_retriever or not langgraph_app or not azure_creds_valid
    ask_button = st.button("Get Answer", type="primary", use_container_width=True, disabled=ask_button_disabled)

if selected_question == PREDEFINED_QUESTIONS[0]:
    final_question = custom_question
else:
    final_question = selected_question

# --- Answer Generation and Display (Using LangGraph) ---
st.divider()
st.subheader("Answer")

answer_placeholder = st.empty()
context_placeholder = st.empty()

# Only run if button clicked AND prerequisites met
if ask_button and base_retriever and final_question and langgraph_app and azure_creds_valid:
    answer_placeholder.info("Running graph with Azure OpenAI (MultiQuery Retrieval -> Filter -> Grade -> Generate)...", icon="‚è≥")
    context_placeholder.empty()

    try:
        # Define the initial state to pass to the graph
        # Pass the BASE retriever to the state
        initial_state = {"question": final_question, "base_retriever": base_retriever}

        # Invoke the LangGraph app
        final_state = langgraph_app.invoke(initial_state)

        # Extract the final answer
        answer = final_state.get("generation", None)

        if answer:
             answer_placeholder.success(answer)
        else:
            answer_placeholder.warning("Could not find relevant information in the document to answer the question after filtering and grading.", icon="‚ö†Ô∏è")


        # Display the context: Show the unique docs retrieved by MultiQuery
        original_docs_retrieved = final_state.get('original_documents', [])

        if original_docs_retrieved:
            with context_placeholder.expander(f"Show Initially Retrieved Context ({len(original_docs_retrieved)} unique chunks via MultiQuery)"):
                for i, doc in enumerate(original_docs_retrieved):
                    page_num = doc.metadata.get('page', 'N/A')
                    st.markdown(f"**Chunk {i+1} (MultiQuery Retrieval - Page {page_num}):**")
                    st.markdown(f"> {doc.page_content}")
                    st.divider()
        elif final_state.get("generation") is None:
             context_placeholder.info("No documents were retrieved initially by MultiQuery.")


    except Exception as e:
        answer_placeholder.error(f"Error running LangGraph: {e}", icon="‚ùå")
        st.exception(e)

# Handle other button click conditions
elif ask_button and not base_retriever:
    answer_placeholder.warning("Please upload and process a PDF file first (ensure Azure credentials are set).", icon="‚ö†Ô∏è")
elif ask_button and not final_question:
     answer_placeholder.warning("Please enter or select a question.", icon="‚ö†Ô∏è")
elif ask_button and (not langgraph_app or not azure_creds_valid):
     if not azure_creds_valid:
         answer_placeholder.error("Cannot get answer: Azure credentials missing.", icon="üö®")
     elif not langgraph_app:
         answer_placeholder.error("Cannot get answer: LangGraph application failed to compile.", icon="üö®")
# Initial state message
else:
    if azure_creds_valid:
        answer_placeholder.info("Upload a PDF and ask a question to get started.")


# --- Footer ---
st.divider()
st.caption("Powered by LangChain, LangGraph, Azure OpenAI, FAISS, and Streamlit")
