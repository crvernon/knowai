{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a7380d0-476b-4134-81d8-e851a2b1e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated, Sequence, Dict, Optional, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_community.vectorstores import FAISS #, VectorStoreRetriever\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7188b8f2-5383-48da-be08-12de8e913dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"/Users/d3y010/repos/crvernon/knowai\"\n",
    "\n",
    "vectorstore_path = \"/Users/d3y010/repos/crvernon/knowai/test_faiss_store\"\n",
    "\n",
    "load_dotenv(os.path.join(root_directory, \".env\")) # Load environment variables from .env file\n",
    "\n",
    "k_chunks_retriever = 25\n",
    "\n",
    "# Fetch Azure credentials from environment variables\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", default=None)\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", default=None)\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\") # Default model\n",
    "embeddings_deployment = os.getenv(\"AZURE_EMBEDDINGS_DEPLOYMENT\", \"text-embedding-3-large\") # Default embedding model\n",
    "openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\") # API version\n",
    "\n",
    "# instantiate logger\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fcea2ac8-eb08-441a-850d-82e2d5d9dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def list_vectorstore_files(vectorstore) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return a sorted list of unique filenames stored in the FAISS vectorstore.\n",
    "\n",
    "    The filenames are extracted from the ``metadata`` dictionary of each\n",
    "    document under the key ``\"file\"``. If the vectorstore is ``None`` or no\n",
    "    filenames are present, an empty list is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorstore : FAISS | None\n",
    "        The FAISS vectorstore instance from which to extract filenames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        Alphabetically sorted list of unique filenames found in the metadata.\n",
    "    \"\"\"\n",
    "    if vectorstore is None:\n",
    "        logging.error(\"Cannot list files: vectorstore is None\")\n",
    "        return []\n",
    "\n",
    "    files = set()\n",
    "    # Access the underlying docstore dictionary\n",
    "    try:\n",
    "        for _, doc in vectorstore.docstore._dict.items():\n",
    "            filename = doc.metadata.get(\"file\")\n",
    "            if filename:\n",
    "                files.add(filename)\n",
    "    except Exception:\n",
    "        logging.exception(\"Failed to access docstore when listing files\")\n",
    "        return []\n",
    "\n",
    "    file_list = sorted(files)\n",
    "    logging.info(\"Files in vectorstore: %s\", file_list)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def pdf_to_chunks(\n",
    "    file_path: str,\n",
    "    chunk_size: int = 1500,\n",
    "    chunk_overlap: int = 300,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single PDF file and split its pages into overlapping text chunks.\n",
    "\n",
    "    Each chunk is returned as a ``Document`` whose ``metadata`` dictionary\n",
    "    contains:\n",
    "        {\"file\": \"<pdf-filename>\", \"page\": <page-number>}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the PDF file.\n",
    "    chunk_size : int, default 1500\n",
    "        Maximum characters per chunk.\n",
    "    chunk_overlap : int, default 300\n",
    "        Number of characters overlapped between adjacent chunks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Document]\n",
    "        ``Document`` objects ready for vectorstore ingestion.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(\"PDF file does not exist: %s\", file_path)\n",
    "        return []\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    chunks: List[Document] = []\n",
    "    try:\n",
    "        pdf = fitz.open(file_path)\n",
    "    except Exception:\n",
    "        logging.exception(\"Failed to open PDF: %s\", file_path)\n",
    "        return []\n",
    "\n",
    "    for page_num in range(len(pdf)):\n",
    "        try:\n",
    "            text = pdf.load_page(page_num).get_text()\n",
    "        except Exception:\n",
    "            logging.exception(\"Failed to read page %d of %s\", page_num + 1, filename)\n",
    "            continue\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        for piece in splitter.split_text(text):\n",
    "            chunks.append(\n",
    "                Document(\n",
    "                    page_content=piece,\n",
    "                    metadata={\"file\": filename, \"page\": page_num + 1},\n",
    "                )\n",
    "            )\n",
    "\n",
    "    pdf.close()\n",
    "    logging.info(\"Extracted %d chunks from %s\", len(chunks), filename)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def add_new_pdfs_to_vectorstore(\n",
    "    directory_path: str,\n",
    "    vectorstore: FAISS,\n",
    "    chunk_size: int = 1500,\n",
    "    chunk_overlap: int = 300,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Scan a directory for PDF files that are not yet present in the given\n",
    "    vectorstore and add their text chunks to the store.\n",
    "\n",
    "    A PDF is considered *already present* if its filename appears in the\n",
    "    ``\"file\"`` metadata of any existing document in the vectorstore.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Directory containing one or more ``.pdf`` files.\n",
    "    vectorstore : FAISS\n",
    "        An *already instantiated* FAISS vectorstore to update.\n",
    "    chunk_size : int, default 1500\n",
    "        Maximum characters per chunk when splitting text.\n",
    "    chunk_overlap : int, default 300\n",
    "        Number of characters overlapped between consecutive chunks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Total number of **new chunks** added to the vectorstore.\n",
    "    \"\"\"\n",
    "    if vectorstore is None:\n",
    "        logging.error(\"Vectorstore is None; cannot add new PDFs.\")\n",
    "        return 0\n",
    "\n",
    "    if not os.path.isdir(directory_path):\n",
    "        logging.error(\"Provided directory does not exist: %s\", directory_path)\n",
    "        return 0\n",
    "\n",
    "    # Collect filenames already indexed\n",
    "    existing_files = set()\n",
    "    for _, doc in vectorstore.docstore._dict.items():\n",
    "        filename = doc.metadata.get(\"file\")\n",
    "        if filename:\n",
    "            existing_files.add(filename)\n",
    "\n",
    "    new_docs: List[Document] = []\n",
    "    pdf_files = sorted(\n",
    "        f for f in os.listdir(directory_path) if f.lower().endswith(\".pdf\")\n",
    "    )\n",
    "\n",
    "    for filename in pdf_files:\n",
    "        if filename in existing_files:\n",
    "            logging.debug(\"Skipping %s (already indexed).\", filename)\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        docs = pdf_to_chunks(\n",
    "            file_path=file_path,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        new_docs.extend(docs)\n",
    "\n",
    "    if not new_docs:\n",
    "        logging.info(\"No new PDFs found in %s.\", directory_path)\n",
    "        return 0\n",
    "\n",
    "    vectorstore.add_documents(new_docs)\n",
    "    logging.info(\n",
    "        \"Added %d chunks from %d new PDF(s) to vectorstore.\",\n",
    "        len(new_docs),\n",
    "        len({d.metadata['file'] for d in new_docs}),\n",
    "    )\n",
    "    return len(new_docs)\n",
    "\n",
    "\n",
    "def instantiate_retriever(\n",
    "    vectorstore: FAISS,\n",
    "    allowed_files: Optional[List[str]] = None,\n",
    "    top_n_similar: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    Instantiate a retriever from an existing FAISS vectorstore.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorstore : FAISS\n",
    "        The FAISS vectorstore that already contains your document embeddings.\n",
    "    allowed_files : list[str] | None, optional\n",
    "        Filenames to restrict retrieval to. When ``None`` or an empty list,\n",
    "        the retriever searches across **all** chunks in the store.\n",
    "    top_n_similar : int, default 25\n",
    "        Number of most‑similar chunks to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    VectorStoreRetriever\n",
    "        A LangChain retriever configured with the specified constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If *vectorstore* is ``None``.\n",
    "    \"\"\"\n",
    "    if vectorstore is None:\n",
    "        raise ValueError(\"`vectorstore` must be a valid FAISS instance.\")\n",
    "\n",
    "    # Base kwargs\n",
    "    search_kwargs = {\"k\": top_n_similar}\n",
    "\n",
    "    # Add metadata filter only when filenames are supplied\n",
    "    if allowed_files:\n",
    "        search_kwargs[\"filter\"] = {\"file\": {\"$in\": allowed_files}}\n",
    "\n",
    "    return vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
    "\n",
    "\n",
    "def extract_using_multiquery(\n",
    "    question: str,\n",
    "    llm: \"BaseLanguageModel\",\n",
    "    retriever: \"VectorStoreRetriever\",\n",
    "    n_alternatives: int = 4,\n",
    "    k_per_query: int = 25,\n",
    ") -> List[\"Document\"]:\n",
    "    \"\"\"\n",
    "    Generate alternative phrasings of *question* (via ``MultiQueryRetriever``'s components)\n",
    "    **and** return the unique chunks retrieved from the provided *retriever*.\n",
    "\n",
    "    The function workflow is:\n",
    "\n",
    "    1. Use the ``llm_chain`` from ``MultiQueryRetriever`` to generate a text containing\n",
    "       multiple query formulations.\n",
    "    2. Parse this text (typically newline-separated queries) into a list of queries.\n",
    "    3. For each generated query (plus the original), run\n",
    "       ``retriever.get_relevant_documents`` (with *k_per_query*).\n",
    "    4. Merge all returned `Document`s, deduplicate them\n",
    "       (file‑name + page‑number + content), and return the unique list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The original user question.\n",
    "    llm : BaseLanguageModel\n",
    "        The LLM used by ``MultiQueryRetriever`` to craft alternative queries.\n",
    "    retriever : VectorStoreRetriever\n",
    "        A retriever tied to the target vectorstore.\n",
    "    n_alternatives : int, default 4\n",
    "        Number of alternative queries to generate.\n",
    "    k_per_query : int, default 25\n",
    "        Top‑*k* chunks to retrieve per query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Document]\n",
    "        Unique chunks drawn from the vectorstore across **all** generated\n",
    "        queries (including the original question).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The function silently falls back to returning chunks for just the\n",
    "      original question if alternative query generation fails.\n",
    "    * Deduplication key: ``(file, page, page_content)``.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize MultiQueryRetriever to access its llm_chain\n",
    "    # Note: Ensure the MultiQueryRetriever import path is correct for your Langchain version.\n",
    "    mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "    alt_queries: List[str] = []\n",
    "    try:\n",
    "        # Directly use the llm_chain from MultiQueryRetriever to generate queries.\n",
    "        # The input to the llm_chain is typically a dict with the key \"question\".\n",
    "        chain_input = {\"question\": question}\n",
    "        # Invoke the chain; it will use default callback handling.\n",
    "        llm_response = mqr.llm_chain.invoke(chain_input)\n",
    "\n",
    "        raw_queries_text = \"\"\n",
    "        # Process the llm_response to get a single string of newline-separated queries.\n",
    "        if isinstance(llm_response, dict):\n",
    "            # Default output key for LLMChain is 'text'.\n",
    "            # MultiQueryRetriever's llm_chain is an LLMChain.\n",
    "            raw_queries_text = llm_response.get(mqr.llm_chain.output_key, \"\")\n",
    "            logging.debug(f\"LLMChain response was a dict. Raw text: '{raw_queries_text[:100]}...'\")\n",
    "        elif isinstance(llm_response, list):\n",
    "            logging.debug(\n",
    "                f\"LLMChain response was a list. Attempting to process as list of query strings. Content: {llm_response}\"\n",
    "            )\n",
    "            # If the LLM directly returns a list of query strings\n",
    "            if all(isinstance(item, str) for item in llm_response):\n",
    "                raw_queries_text = \"\\n\".join(llm_response) # Join them into a single string\n",
    "                logging.debug(f\"Joined list of strings into: '{raw_queries_text[:100]}...'\")\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    \"LLMChain response was a list, but not all items are strings. Cannot process.\"\n",
    "                )\n",
    "        elif isinstance(llm_response, str):\n",
    "            logging.debug(\n",
    "                f\"LLMChain response was a string directly. Processing as raw text. Response: {llm_response[:100]}...\"\n",
    "            )\n",
    "            raw_queries_text = llm_response\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"Unexpected response type from llm_chain.invoke: {type(llm_response)}. Expected dict, list, or str.\"\n",
    "            )\n",
    "\n",
    "        if raw_queries_text:\n",
    "            # The default prompt for MultiQueryRetriever asks for newline-separated queries.\n",
    "            # Split the raw text by newlines and filter out any empty strings.\n",
    "            alt_queries = [q.strip() for q in raw_queries_text.split(\"\\n\") if q.strip()]\n",
    "            logging.debug(f\"Parsed queries from raw text: {alt_queries}\")\n",
    "        else:\n",
    "            logging.warning(\"No raw query text obtained or processed from LLM chain response for multi-query.\")\n",
    "            alt_queries = []\n",
    "\n",
    "    except Exception:\n",
    "        logging.exception(\n",
    "            \"Failed to generate or parse alternative queries using mqr.llm_chain.invoke\"\n",
    "        )\n",
    "        alt_queries = [] # Ensure alt_queries is defined and empty on failure\n",
    "\n",
    "    # Clean & truncate to desired count (duplicates are already handled by list(dict.fromkeys(...)))\n",
    "    # Remove empty strings (already done by strip() and check in list comprehension)\n",
    "    # and duplicates, then truncate.\n",
    "    alt_queries = list(dict.fromkeys(alt_queries)) # Remove duplicates while preserving order\n",
    "    alt_queries = alt_queries[:n_alternatives]\n",
    "\n",
    "    # Always include the original question for retrieval\n",
    "    query_list = [question] + alt_queries\n",
    "\n",
    "    logging.info(f\"Running retrieval for {len(query_list)} queries: {query_list}\")\n",
    "\n",
    "    # Retrieve docs for each query\n",
    "    retrieved_docs: List[\"Document\"] = []\n",
    "    for q_text in query_list:\n",
    "        try:\n",
    "            # Attempt to use 'k' parameter\n",
    "            docs_for_query = retriever.get_relevant_documents(q_text, k=k_per_query)\n",
    "        except TypeError as e:\n",
    "            # Check if TypeError is due to unexpected 'k' argument\n",
    "            if 'unexpected keyword argument \\'k\\'' in str(e).lower() or \\\n",
    "               'got an unexpected keyword argument \\'k\\'' in str(e).lower():\n",
    "                logging.debug(f\"Retriever for query '{q_text[:50]}...' does not support 'k' arg, retrieving all and slicing.\")\n",
    "                docs_for_query = retriever.get_relevant_documents(q_text)\n",
    "                docs_for_query = docs_for_query[:k_per_query] # Manual slicing\n",
    "            else:\n",
    "                # Different TypeError, log and skip this query's docs\n",
    "                logging.exception(f\"TypeError during retrieval for query: '{q_text[:50]}...'\")\n",
    "                continue\n",
    "        except Exception:\n",
    "            logging.exception(f\"Retrieval failed for query: '{q_text[:50]}...'\")\n",
    "            continue # Skip this query's docs if any other exception occurs\n",
    "        retrieved_docs.extend(docs_for_query)\n",
    "\n",
    "    # Deduplicate documents\n",
    "    # Using a dictionary to store unique documents based on a key\n",
    "    unique_docs_map: Dict[tuple, \"Document\"] = {}\n",
    "    for doc in retrieved_docs:\n",
    "        # Create a unique key for each document.\n",
    "        # Ensure page_content is stripped for consistent keying.\n",
    "        # Handle cases where metadata might be missing 'file' or 'page'.\n",
    "        doc_file = doc.metadata.get(\"file\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "        doc_page = doc.metadata.get(\"page\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "        key = (\n",
    "            doc_file,\n",
    "            doc_page,\n",
    "            doc.page_content.strip() if hasattr(doc, 'page_content') else \"\",\n",
    "        )\n",
    "        if key not in unique_docs_map:\n",
    "            unique_docs_map[key] = doc\n",
    "\n",
    "    final_unique_docs = list(unique_docs_map.values())\n",
    "    logging.info(f\"Retrieved {len(retrieved_docs)} chunks -> {len(final_unique_docs)} unique after dedup.\")\n",
    "\n",
    "    return final_unique_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "963c247f-2fc3-4307-92a6-60788e86659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings model\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embeddings_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    openai_api_version=openai_api_version\n",
    ")\n",
    "\n",
    "# load existing vectorstore\n",
    "vectorstore = load_faiss_vectorstore(\n",
    "    path=vectorstore_path, \n",
    "    embeddings=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee00c2ca-5324-4984-b739-70c530eb71e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID_OR_Idaho_Power_2022.pdf', 'OR_City_of_Bandon_2024.pdf']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files currently in the vectorstore\n",
    "files_in_vectorstore = list_vectorstore_files(vectorstore)\n",
    "\n",
    "files_in_vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fff6440-a93e-4ff7-bc57-719f09b7f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new files from a directory\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "668d9584-a21a-47bd-ae8c-39c1b4150401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a retriever\n",
    "retriever = instantiate_retriever(\n",
    "    vectorstore=vectorstore,\n",
    "    allowed_files=[\"OR_City_of_Bandon_2024.pdf\"],\n",
    "    top_n_similar=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d20885b1-4078-45b4-a3e8-19223cb67e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does types of vegetation management are mentioned in this report?\"\n",
    "\n",
    "# retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d783616-c8f5-4e14-84d5-7094c878258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.3, \n",
    "    api_key=api_key, \n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_deployment=deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e301e5d-51db-4311-8863-b00239ed64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = extract_using_multiquery(\n",
    "    question=question,\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    n_alternatives=4,\n",
    "    k_per_query=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d79b1d48-062d-4062-80f4-e99026316a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c3a99-bf10-4e77-890e-88ff3d1b4940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ad0c69a-64d5-492b-ac3d-0ec95f4409b6",
   "metadata": {},
   "source": [
    "## Implement Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "32a23943-5dcd-4ee3-abcb-233aaf3092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "\n",
    "    embeddings: Union[None, AzureOpenAIEmbeddings]\n",
    "    vectorstore_path: str\n",
    "    vectorstore: Union[None, FAISS]\n",
    "    llm_large: Union[None, AzureChatOpenAI]\n",
    "    allowed_files: Union[None, List[str]]\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    individual_answers: Dict[str, str]\n",
    "\n",
    "\n",
    "def instantiate_embeddings(state: GraphState):\n",
    "\n",
    "    # Retrieve current embeddings instantiation state\n",
    "    embeddings = state.get(\"embeddings\", None) \n",
    "\n",
    "    if embeddings:\n",
    "        logging.info(\"Using pre-instantiated embeddings model\")\n",
    "        return state\n",
    "    \n",
    "    else:\n",
    "        logging.info(\"Instantiating embeddings model\")\n",
    "        return {\n",
    "            **state, \n",
    "            \"embeddings\": AzureOpenAIEmbeddings(\n",
    "                azure_deployment=embeddings_deployment,\n",
    "                azure_endpoint=azure_endpoint,\n",
    "                api_key=api_key,\n",
    "                openai_api_version=openai_api_version\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "def instantiate_llm_large(state: GraphState):\n",
    "\n",
    "    # Retrieve current LLM instantiation state\n",
    "    llm_large = state.get(\"llm_large\", None) \n",
    "\n",
    "    if llm_large:\n",
    "        logging.info(\"Using pre-instantiated large LLM model\")\n",
    "        return state\n",
    "    \n",
    "    else:\n",
    "        logging.info(\"Instantiating large LLM model\")\n",
    "        return {\n",
    "            **state, \n",
    "            \"llm_large\": AzureChatOpenAI(\n",
    "                temperature=0, \n",
    "                api_key=api_key, \n",
    "                openai_api_version=openai_api_version,\n",
    "                azure_deployment=deployment,\n",
    "                azure_endpoint=azure_endpoint,\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "def instantiate_retriever(state: GraphState, top_n_similar: int = 25):\n",
    "    \"\"\"\n",
    "    Instantiate a retriever from an existing FAISS vectorstore.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorstore : FAISS\n",
    "        The FAISS vectorstore that already contains your document embeddings.\n",
    "    allowed_files : list[str] | None, optional\n",
    "        Filenames to restrict retrieval to. When ``None`` or an empty list,\n",
    "        the retriever searches across **all** chunks in the store.\n",
    "    top_n_similar : int, default 25\n",
    "        Number of most‑similar chunks to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    VectorStoreRetriever\n",
    "        A LangChain retriever configured with the specified constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If *vectorstore* is ``None``.\n",
    "    \"\"\"\n",
    "    vectorstore = state.get(\"vectorstore\", None)\n",
    "    allowed_files = state.get(\"allowed_files\", None)\n",
    "    \n",
    "    if vectorstore is None:\n",
    "        raise ValueError(\"`vectorstore` must be a valid FAISS instance.\")\n",
    "\n",
    "    # Base kwargs\n",
    "    search_kwargs = {\"k\": top_n_similar}\n",
    "\n",
    "    # Add metadata filter only when filenames are supplied\n",
    "    if allowed_files:\n",
    "        search_kwargs[\"filter\"] = {\"file\": {\"$in\": allowed_files}}\n",
    "\n",
    "    return vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
    "\n",
    "    return {\n",
    "        **state, \n",
    "        \"retriever\": vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
    "    }\n",
    "\n",
    "\n",
    "def load_faiss_vectorstore(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Load a FAISS vectorstore that was previously saved to disk.\n",
    "    Updates the 'vectorstore' field in the state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : GraphState\n",
    "        The current state of the graph, containing 'vectorstore_path' and 'embeddings'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GraphState\n",
    "        The updated state. If loading fails, 'vectorstore' in the state will be None.\n",
    "    \"\"\"\n",
    "    # Retrieve state variables\n",
    "    current_vectorstore_path = state.get(\"vectorstore_path\")\n",
    "    # The 'vectorstore' from the state is the one we might be replacing or confirming.\n",
    "    # No need to get it here as we are deciding whether to load a new one.\n",
    "    embeddings = state.get(\"embeddings\") \n",
    "\n",
    "    # Check if vectorstore is already loaded in a previous step (though this node is usually for loading)\n",
    "    if state.get(\"vectorstore\"):\n",
    "        logging.info(\"Vectorstore already exists in state. Skipping loading.\")\n",
    "        return state # Return the current state as is\n",
    "\n",
    "    if not current_vectorstore_path:\n",
    "        logging.error(\"Vectorstore path is not provided in the state.\")\n",
    "        return {**state, \"vectorstore\": None} # Update state and return\n",
    "\n",
    "    if not os.path.exists(current_vectorstore_path):\n",
    "        logging.error(\"FAISS vectorstore path does not exist: %s\", current_vectorstore_path)\n",
    "        return {**state, \"vectorstore\": None} # Update state and return\n",
    "\n",
    "    if not os.path.isdir(current_vectorstore_path):\n",
    "        logging.error(\"Provided FAISS vectorstore path is not a directory: %s\", current_vectorstore_path)\n",
    "        return {**state, \"vectorstore\": None} # Update state and return\n",
    "    \n",
    "    if embeddings is None:\n",
    "        logging.error(\"Embeddings not yet instantiated. Review workflow to ensure 'instantiate_embeddings' node runs before this.\")\n",
    "        return {**state, \"vectorstore\": None} # Update state and return\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Loading FAISS vectorstore from '%s' ...\", current_vectorstore_path)\n",
    "        loaded_vectorstore = FAISS.load_local(\n",
    "            folder_path=current_vectorstore_path, # Parameter name is folder_path\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        logging.info(\"FAISS vectorstore loaded with %d embeddings.\", loaded_vectorstore.index.ntotal)\n",
    "        return {**state, \"vectorstore\": loaded_vectorstore} # Update state with loaded vectorstore\n",
    "        \n",
    "    except Exception as e: # Catching specific exception is better if known\n",
    "        logging.exception(\"Failed to load FAISS vectorstore from '%s': %s\", current_vectorstore_path, e)\n",
    "        return {**state, \"vectorstore\": None} # Update state, vectorstore is None\n",
    "\n",
    "\n",
    "def extract_using_multiquery(state: GraphState) -> List[\"Document\"]:\n",
    "    \"\"\"\n",
    "    Generate alternative phrasings of *question* (via ``MultiQueryRetriever``'s components)\n",
    "    **and** return the unique chunks retrieved from the provided *retriever*.\n",
    "\n",
    "    The function workflow is:\n",
    "\n",
    "    1. Use the ``llm_chain`` from ``MultiQueryRetriever`` to generate a text containing\n",
    "       multiple query formulations.\n",
    "    2. Parse this text (typically newline-separated queries) into a list of queries.\n",
    "    3. For each generated query (plus the original), run\n",
    "       ``retriever.get_relevant_documents`` (with *k_per_query*).\n",
    "    4. Merge all returned `Document`s, deduplicate them\n",
    "       (file‑name + page‑number + content), and return the unique list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The original user question.\n",
    "    llm : BaseLanguageModel\n",
    "        The LLM used by ``MultiQueryRetriever`` to craft alternative queries.\n",
    "    retriever : VectorStoreRetriever\n",
    "        A retriever tied to the target vectorstore.\n",
    "    n_alternatives : int, default 4\n",
    "        Number of alternative queries to generate.\n",
    "    k_per_query : int, default 25\n",
    "        Top‑*k* chunks to retrieve per query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Document]\n",
    "        Unique chunks drawn from the vectorstore across **all** generated\n",
    "        queries (including the original question).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The function silently falls back to returning chunks for just the\n",
    "      original question if alternative query generation fails.\n",
    "    * Deduplication key: ``(file, page, page_content)``.\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", None)\n",
    "    llm = state.get(\"llm_large\", None)\n",
    "    retriever = state.get(\"retriever\", None)\n",
    "    n_alternatives = state.get(\"n_alternatives\", 4)\n",
    "    k_per_query = state.get(\"k_per_query\", 25)\n",
    "\n",
    "\n",
    "    # Initialize MultiQueryRetriever to access its llm_chain\n",
    "    # Note: Ensure the MultiQueryRetriever import path is correct for your Langchain version.\n",
    "    mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "    alt_queries: List[str] = []\n",
    "    try:\n",
    "        # Directly use the llm_chain from MultiQueryRetriever to generate queries.\n",
    "        # The input to the llm_chain is typically a dict with the key \"question\".\n",
    "        chain_input = {\"question\": question}\n",
    "        # Invoke the chain; it will use default callback handling.\n",
    "        llm_response = mqr.llm_chain.invoke(chain_input)\n",
    "\n",
    "        raw_queries_text = \"\"\n",
    "        # Process the llm_response to get a single string of newline-separated queries.\n",
    "        if isinstance(llm_response, dict):\n",
    "            # Default output key for LLMChain is 'text'.\n",
    "            # MultiQueryRetriever's llm_chain is an LLMChain.\n",
    "            raw_queries_text = llm_response.get(mqr.llm_chain.output_key, \"\")\n",
    "            logging.debug(f\"LLMChain response was a dict. Raw text: '{raw_queries_text[:100]}...'\")\n",
    "        elif isinstance(llm_response, list):\n",
    "            logging.debug(\n",
    "                f\"LLMChain response was a list. Attempting to process as list of query strings. Content: {llm_response}\"\n",
    "            )\n",
    "            # If the LLM directly returns a list of query strings\n",
    "            if all(isinstance(item, str) for item in llm_response):\n",
    "                raw_queries_text = \"\\n\".join(llm_response) # Join them into a single string\n",
    "                logging.debug(f\"Joined list of strings into: '{raw_queries_text[:100]}...'\")\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    \"LLMChain response was a list, but not all items are strings. Cannot process.\"\n",
    "                )\n",
    "        elif isinstance(llm_response, str):\n",
    "            logging.debug(\n",
    "                f\"LLMChain response was a string directly. Processing as raw text. Response: {llm_response[:100]}...\"\n",
    "            )\n",
    "            raw_queries_text = llm_response\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"Unexpected response type from llm_chain.invoke: {type(llm_response)}. Expected dict, list, or str.\"\n",
    "            )\n",
    "\n",
    "        if raw_queries_text:\n",
    "            # The default prompt for MultiQueryRetriever asks for newline-separated queries.\n",
    "            # Split the raw text by newlines and filter out any empty strings.\n",
    "            alt_queries = [q.strip() for q in raw_queries_text.split(\"\\n\") if q.strip()]\n",
    "            logging.debug(f\"Parsed queries from raw text: {alt_queries}\")\n",
    "        else:\n",
    "            logging.warning(\"No raw query text obtained or processed from LLM chain response for multi-query.\")\n",
    "            alt_queries = []\n",
    "\n",
    "    except Exception:\n",
    "        logging.exception(\n",
    "            \"Failed to generate or parse alternative queries using mqr.llm_chain.invoke\"\n",
    "        )\n",
    "        alt_queries = [] # Ensure alt_queries is defined and empty on failure\n",
    "\n",
    "    # Clean & truncate to desired count (duplicates are already handled by list(dict.fromkeys(...)))\n",
    "    # Remove empty strings (already done by strip() and check in list comprehension)\n",
    "    # and duplicates, then truncate.\n",
    "    alt_queries = list(dict.fromkeys(alt_queries)) # Remove duplicates while preserving order\n",
    "    alt_queries = alt_queries[:n_alternatives]\n",
    "\n",
    "    # Always include the original question for retrieval\n",
    "    query_list = [question] + alt_queries\n",
    "\n",
    "    logging.info(f\"Running retrieval for {len(query_list)} queries: {query_list}\")\n",
    "\n",
    "    # Retrieve docs for each query\n",
    "    retrieved_docs: List[\"Document\"] = []\n",
    "    for q_text in query_list:\n",
    "        try:\n",
    "            # Attempt to use 'k' parameter\n",
    "            docs_for_query = retriever.get_relevant_documents(q_text, k=k_per_query)\n",
    "        except TypeError as e:\n",
    "            # Check if TypeError is due to unexpected 'k' argument\n",
    "            if 'unexpected keyword argument \\'k\\'' in str(e).lower() or \\\n",
    "               'got an unexpected keyword argument \\'k\\'' in str(e).lower():\n",
    "                logging.debug(f\"Retriever for query '{q_text[:50]}...' does not support 'k' arg, retrieving all and slicing.\")\n",
    "                docs_for_query = retriever.get_relevant_documents(q_text)\n",
    "                docs_for_query = docs_for_query[:k_per_query] # Manual slicing\n",
    "            else:\n",
    "                # Different TypeError, log and skip this query's docs\n",
    "                logging.exception(f\"TypeError during retrieval for query: '{q_text[:50]}...'\")\n",
    "                continue\n",
    "        except Exception:\n",
    "            logging.exception(f\"Retrieval failed for query: '{q_text[:50]}...'\")\n",
    "            continue # Skip this query's docs if any other exception occurs\n",
    "        retrieved_docs.extend(docs_for_query)\n",
    "\n",
    "    # Deduplicate documents\n",
    "    # Using a dictionary to store unique documents based on a key\n",
    "    unique_docs_map: Dict[tuple, \"Document\"] = {}\n",
    "    for doc in retrieved_docs:\n",
    "        # Create a unique key for each document.\n",
    "        # Ensure page_content is stripped for consistent keying.\n",
    "        # Handle cases where metadata might be missing 'file' or 'page'.\n",
    "        doc_file = doc.metadata.get(\"file\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "        doc_page = doc.metadata.get(\"page\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "        key = (\n",
    "            doc_file,\n",
    "            doc_page,\n",
    "            doc.page_content.strip() if hasattr(doc, 'page_content') else \"\",\n",
    "        )\n",
    "        if key not in unique_docs_map:\n",
    "            unique_docs_map[key] = doc\n",
    "\n",
    "    final_unique_docs = list(unique_docs_map.values())\n",
    "    logging.info(f\"Retrieved {len(retrieved_docs)} chunks -> {len(final_unique_docs)} unique after dedup.\")\n",
    "\n",
    "    return final_unique_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3a11b1a-6738-4d19-9d08-cc6ce9c2241b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 214\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(os.path.join(root_directory, \u001b[33m\"\u001b[39m\u001b[33m.env\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    212\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.env file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.join(root_directory,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.env\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Azure credentials might be missing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/asyncio/runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def _async_extract_for_single_file(\n",
    "    question: str, \n",
    "    llm: BaseLanguageModel, \n",
    "    retriever: \"VectorStoreRetriever\", # File-specific retriever\n",
    "    file_name: str, \n",
    "    n_alternatives: int, \n",
    "    k_per_query: int\n",
    ") -> tuple[str, Optional[List[Document]]]:\n",
    "    \"\"\"\n",
    "    Async helper to extract documents for a single file using MultiQueryRetriever.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting extraction for file: {file_name} with question: \\\"{question[:50]}...\\\"\")\n",
    "    try:\n",
    "        # Ensure retriever is file-specific by checking its search_kwargs if possible\n",
    "        # This function assumes the passed retriever is already correctly filtered for the file_name\n",
    "        \n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "        alt_queries: List[str] = []\n",
    "        \n",
    "        chain_input = {\"question\": question}\n",
    "        llm_response = await mqr.llm_chain.ainvoke(chain_input) \n",
    "        \n",
    "        raw_queries_text = \"\"\n",
    "        if isinstance(llm_response, dict):\n",
    "            raw_queries_text = llm_response.get(mqr.llm_chain.output_key, \"\")\n",
    "        elif isinstance(llm_response, str):\n",
    "            raw_queries_text = llm_response\n",
    "        else:\n",
    "            logging.warning(f\"[{file_name}] Unexpected LLM response type: {type(llm_response)}. Trying to parse as string.\")\n",
    "            raw_queries_text = str(llm_response)\n",
    "\n",
    "        if raw_queries_text:\n",
    "            alt_queries = [q.strip() for q in raw_queries_text.split(\"\\n\") if q.strip()]\n",
    "        else:\n",
    "            logging.warning(f\"[{file_name}] No raw query text obtained from LLM chain response.\")\n",
    "            alt_queries = []\n",
    "\n",
    "        alt_queries = list(dict.fromkeys(alt_queries))[:n_alternatives]\n",
    "        query_list = [question] + alt_queries\n",
    "        logging.debug(f\"[{file_name}] Running retrieval for {len(query_list)} queries: {query_list}\")\n",
    "\n",
    "        retrieved_docs: List[Document] = []\n",
    "        for q_text in query_list:\n",
    "            try:\n",
    "                docs_for_query = await retriever.ainvoke(q_text, k=k_per_query)\n",
    "                retrieved_docs.extend(docs_for_query)\n",
    "            except Exception as e_retrieve:\n",
    "                logging.exception(f\"[{file_name}] Retrieval failed for query: '{q_text[:50]}...': {e_retrieve}\")\n",
    "                continue\n",
    "        \n",
    "        unique_docs_map: Dict[tuple, Document] = {}\n",
    "        for doc in retrieved_docs:\n",
    "            doc_file_meta = doc.metadata.get(\"file\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "            doc_page_meta = doc.metadata.get(\"page\") if hasattr(doc, 'metadata') and doc.metadata else None\n",
    "            key = (\n",
    "                doc_file_meta, \n",
    "                doc_page_meta,\n",
    "                doc.page_content.strip() if hasattr(doc, 'page_content') else \"\",\n",
    "            )\n",
    "            if key not in unique_docs_map:\n",
    "                unique_docs_map[key] = doc\n",
    "        \n",
    "        final_unique_docs = list(unique_docs_map.values())\n",
    "        logging.info(f\"[{file_name}] Retrieved {len(retrieved_docs)} docs -> {len(final_unique_docs)} unique docs.\")\n",
    "        return file_name, final_unique_docs\n",
    "\n",
    "    except Exception as e_outer:\n",
    "        logging.exception(f\"[{file_name}] Critical error during document extraction for file: {e_outer}\")\n",
    "        return file_name, None\n",
    "\n",
    "\n",
    "async def extract_documents_parallel_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to extract documents in parallel for each allowed file using multi-query.\n",
    "    Results are stored in state[\"documents_by_file\"].\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\")\n",
    "    llm = state.get(\"llm_large\")\n",
    "    base_retriever = state.get(\"retriever\") \n",
    "    vectorstore = state.get(\"vectorstore\") \n",
    "    allowed_files = state.get(\"allowed_files\", [])\n",
    "    \n",
    "    n_alternatives = state.get(\"n_alternatives\") if state.get(\"n_alternatives\") is not None else 4\n",
    "    k_per_query = state.get(\"k_per_query\") if state.get(\"k_per_query\") is not None else k_chunks_retriever\n",
    "\n",
    "    # Critical checks for linear flow; if any of these are None, something went wrong earlier.\n",
    "    if not question:\n",
    "        logging.error(\"Question is missing from state. Cannot extract documents.\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "    if not llm:\n",
    "        logging.error(\"LLM is missing from state. Cannot extract documents.\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "    if not base_retriever: # Base retriever should be instantiated by now\n",
    "        logging.error(\"Base retriever is missing from state. Cannot extract documents.\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "    if not vectorstore: # Vectorstore should be loaded by now\n",
    "        logging.error(\"Vectorstore is missing from state. Cannot create file-specific retrievers.\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "    if not allowed_files:\n",
    "        logging.warning(\"No allowed_files specified. Skipping document extraction.\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "\n",
    "\n",
    "    tasks = []\n",
    "    for file_name in allowed_files:\n",
    "        file_specific_search_kwargs = {**(base_retriever.search_kwargs if base_retriever.search_kwargs else {}), \n",
    "                                       \"filter\": {\"file\": file_name}}\n",
    "        \n",
    "        try:\n",
    "            file_specific_retriever = vectorstore.as_retriever(search_kwargs=file_specific_search_kwargs)\n",
    "            logging.debug(f\"Created file-specific retriever for '{file_name}' with kwargs: {file_specific_search_kwargs}\")\n",
    "        except Exception as e_ret_create:\n",
    "            logging.error(f\"Failed to create file-specific retriever for {file_name}: {e_ret_create}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        tasks.append(\n",
    "            _async_extract_for_single_file(\n",
    "                question=question,\n",
    "                llm=llm,\n",
    "                retriever=file_specific_retriever,\n",
    "                file_name=file_name,\n",
    "                n_alternatives=n_alternatives,\n",
    "                k_per_query=k_per_query\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if not tasks:\n",
    "        logging.warning(\"No tasks created for parallel extraction (e.g., issues creating retrievers for all allowed_files).\")\n",
    "        return {**state, \"documents_by_file\": {}}\n",
    "\n",
    "    logging.info(f\"Gathering results for {len(tasks)} files asynchronously...\")\n",
    "    results_with_filenames = await asyncio.gather(*tasks)\n",
    "\n",
    "    documents_by_file_result: Dict[str, List[Document]] = {}\n",
    "    for f_name, docs in results_with_filenames:\n",
    "        if docs is not None:\n",
    "            documents_by_file_result[f_name] = docs\n",
    "            logging.info(f\"Successfully processed and stored {len(docs)} documents for file: {f_name}\")\n",
    "        else:\n",
    "            documents_by_file_result[f_name] = [] \n",
    "            logging.warning(f\"Processing failed for file: {f_name}. Storing empty list.\")\n",
    "\n",
    "    return {**state, \"documents_by_file\": documents_by_file_result}\n",
    "\n",
    "\n",
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"instantiate_embeddings_node\", instantiate_embeddings)\n",
    "workflow.add_node(\"instantiate_llm_node\", instantiate_llm_large)\n",
    "workflow.add_node(\"load_vectorstore_node\", load_faiss_vectorstore)\n",
    "workflow.add_node(\"instantiate_retriever_node\", instantiate_retriever)\n",
    "workflow.add_node(\"extract_documents_node\", extract_documents_parallel_node)\n",
    "\n",
    "# Define the graph's flow (edges) - Linear sequence\n",
    "workflow.set_entry_point(\"instantiate_embeddings_node\")\n",
    "workflow.add_edge(\"instantiate_embeddings_node\", \"instantiate_llm_node\")\n",
    "workflow.add_edge(\"instantiate_llm_node\", \"load_vectorstore_node\")\n",
    "workflow.add_edge(\"load_vectorstore_node\", \"instantiate_retriever_node\")\n",
    "workflow.add_edge(\"instantiate_retriever_node\", \"extract_documents_node\")\n",
    "workflow.add_edge(\"extract_documents_node\", END) # After extraction, the process ends\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Define the initial state for the graph invocation\n",
    "initial_state: GraphState = {\n",
    "    \"embeddings\": None,\n",
    "    \"vectorstore_path\": vectorstore_path, \n",
    "    \"vectorstore\": None,\n",
    "    \"llm_large\": None,\n",
    "    \"retriever\": None,\n",
    "    \"allowed_files\": ['ID_OR_Idaho_Power_2022.pdf', 'OR_City_of_Bandon_2024.pdf'], # Example files\n",
    "    \"question\": \"What are the current practices for vegetation management?\", \n",
    "    \"documents_by_file\": None, \n",
    "    \"individual_answers\": None,\n",
    "    \"n_alternatives\": 4,       \n",
    "    \"k_per_query\": 10          \n",
    "}\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main async function to run the graph.\n",
    "    \"\"\"\n",
    "    if not all([api_key, azure_endpoint]):\n",
    "        logging.error(\"Azure OpenAI credentials not found. Halting.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Starting asynchronous graph invocation with linear flow...\")\n",
    "    final_state = await app.ainvoke(initial_state)\n",
    "    \n",
    "    logging.info(\"Graph invocation completed.\")\n",
    "    logging.info(\"Final state overview:\")\n",
    "    logging.info(f\"  Embeddings loaded: {True if final_state.get('embeddings') else False}\")\n",
    "    logging.info(f\"  LLM loaded: {True if final_state.get('llm_large') else False}\")\n",
    "    logging.info(f\"  Vectorstore loaded: {True if final_state.get('vectorstore') else False}\")\n",
    "    if final_state.get('vectorstore'):\n",
    "        logging.info(f\"    Vectorstore contains {final_state['vectorstore'].index.ntotal} embeddings.\")\n",
    "    logging.info(f\"  Retriever instantiated: {True if final_state.get('retriever') else False}\")\n",
    "    \n",
    "    docs_by_file = final_state.get(\"documents_by_file\")\n",
    "    if docs_by_file:\n",
    "        logging.info(\"  Documents extracted by file:\")\n",
    "        for file_name, docs_list in docs_by_file.items():\n",
    "            logging.info(f\"    {file_name}: {len(docs_list)} documents\")\n",
    "    else:\n",
    "        logging.info(\"  No documents were extracted or extraction failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(os.path.join(root_directory, \".env\")):\n",
    "        logging.warning(f\".env file not found at {os.path.join(root_directory, '.env')}. Azure credentials might be missing.\")\n",
    "    \n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aff60df9-4bbe-4c3d-9cd6-13bad9c1c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"instantiate_embeddings\", instantiate_embeddings)\n",
    "workflow.add_node(\"instantiate_llm_large\", instantiate_llm_large)\n",
    "workflow.add_node(\"load_vectorstore\", load_faiss_vectorstore)\n",
    "workflow.add_node(\"instantiate_retriever\", instantiate_retriever)\n",
    "\n",
    "# Define the graph's flow (edges)\n",
    "workflow.set_entry_point(\"instantiate_embeddings\")\n",
    "workflow.set_entry_point(\"instantiate_llm_large\")\n",
    "\n",
    "workflow.add_edge(\"instantiate_embeddings\", \"load_vectorstore\")\n",
    "workflow.add_edge(\"load_vectorstore\", \"instantiate_retriever\") \n",
    "\n",
    "workflow.add_edge(\"load_vectorstore\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3394b98c-b107-49f3-8d60-f03197f29df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial state for the graph invocation\n",
    "initial_state: GraphState = {\n",
    "    \"embeddings\": None,\n",
    "    \"vectorstore_path\": vectorstore_path, \n",
    "    \"vectorstore\": None,\n",
    "    \"llm_large\": None,\n",
    "    \"allowed_files\": None,\n",
    "    \"question\": None, \n",
    "    \"documents\": None,\n",
    "    \"individual_answers\": None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b85f55a-dd31-49c5-8c2a-2f21577fb2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "58134f24-6584-4d2e-890c-cd0c176ac782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_state = asyncio.run(app.ainvoke(initial_state))\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57ec6f99-1e78-4106-a569-c9f1dbe6134f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12535ae90>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.get(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99bb0e-98ad-4e77-8a07-8fc53f16fb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51d051-4349-4326-b4bd-22c74e3b38bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc7022-c020-4da4-9452-c77eaa4bbd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddc21a-81f5-4f78-bba0-38d0a235ddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8593af-f366-4081-94c3-193ef671bcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5edd0-34c6-4369-a307-66f3d948c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a099a8-d88f-420e-a03c-47e0745977ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03325210-ee9f-49f3-ba24-9db4ed2fcea7",
   "metadata": {},
   "source": [
    "## asyncio implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d37b5957-60f9-4b2b-861c-ff16b1906ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time \n",
    "from typing import List, TypedDict, Annotated, Sequence, Dict, Optional, Union\n",
    "from collections import deque # For managing conversation history\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.vectorstores import VectorStoreRetriever \n",
    "from langchain_core.embeddings import Embeddings as LangchainEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "root_directory = \"/Users/d3y010/repos/crvernon/knowai\"\n",
    "vectorstore_path = \"/Users/d3y010/repos/crvernon/knowai/test_faiss_store\"\n",
    "\n",
    "load_dotenv(os.path.join(root_directory, \".env\")) \n",
    "\n",
    "k_chunks_retriever = 25 \n",
    "COMBINE_THRESHOLD = 3 # Max answers/document sets to combine directly; above this, use 2-stage.\n",
    "MAX_CONVERSATION_TURNS = 10 \n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", default=None)\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", default=None)\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o\") \n",
    "embeddings_deployment = os.getenv(\"AZURE_EMBEDDINGS_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    embeddings: Union[None, LangchainEmbeddings] \n",
    "    vectorstore_path: str\n",
    "    vectorstore: Union[None, FAISS]\n",
    "    llm_large: Union[None, AzureChatOpenAI] \n",
    "    retriever: Union[None, VectorStoreRetriever] \n",
    "    allowed_files: Optional[List[str]] \n",
    "    question: Optional[str] \n",
    "    documents_by_file: Optional[Dict[str, List[Document]]] \n",
    "    individual_answers: Optional[Dict[str, str]] \n",
    "    n_alternatives: Optional[int] \n",
    "    k_per_query: Optional[int]\n",
    "    generation: Optional[str] \n",
    "    conversation_history: Optional[List[Dict[str, str]]]\n",
    "    bypass_individual_generation: Optional[bool] # New flag\n",
    "    raw_documents_for_synthesis: Optional[str] # New field for formatted raw docs\n",
    "\n",
    "\n",
    "def instantiate_embeddings(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"instantiate_embeddings_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} ---\")\n",
    "    if not state.get(\"embeddings\"):\n",
    "        logging.info(\"Instantiating embeddings model\")\n",
    "        try:\n",
    "            new_embeddings = AzureOpenAIEmbeddings(\n",
    "                azure_deployment=embeddings_deployment, azure_endpoint=azure_endpoint,\n",
    "                api_key=api_key, openai_api_version=openai_api_version\n",
    "            )\n",
    "            state = {**state, \"embeddings\": new_embeddings}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to instantiate embeddings model: {e}\")\n",
    "            state = {**state, \"embeddings\": None}\n",
    "    else:\n",
    "        logging.info(\"Using pre-instantiated embeddings model\")\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state\n",
    "\n",
    "def instantiate_llm_large(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"instantiate_llm_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} (for query generation) ---\")\n",
    "    if not state.get(\"llm_large\"):\n",
    "        logging.info(\"Instantiating large LLM model (for query generation)\")\n",
    "        try:\n",
    "            new_llm = AzureChatOpenAI(\n",
    "                temperature=0, api_key=api_key, openai_api_version=openai_api_version,\n",
    "                azure_deployment=deployment, azure_endpoint=azure_endpoint,\n",
    "            )\n",
    "            state = {**state, \"llm_large\": new_llm}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to instantiate large LLM model: {e}\")\n",
    "            state = {**state, \"llm_large\": None}\n",
    "    else:\n",
    "        logging.info(\"Using pre-instantiated large LLM model (for query generation)\")\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state\n",
    "\n",
    "def load_faiss_vectorstore(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"load_vectorstore_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} ---\")\n",
    "    current_vectorstore_path = state.get(\"vectorstore_path\")\n",
    "    embeddings = state.get(\"embeddings\")\n",
    "    if \"vectorstore\" not in state: state[\"vectorstore\"] = None\n",
    "    if state.get(\"vectorstore\"): logging.info(\"Vectorstore already exists in state.\")\n",
    "    elif not current_vectorstore_path: logging.error(\"Vectorstore path not provided.\"); state[\"vectorstore\"] = None\n",
    "    elif not embeddings: logging.error(\"Embeddings not instantiated.\"); state[\"vectorstore\"] = None\n",
    "    elif not os.path.exists(current_vectorstore_path) or not os.path.isdir(current_vectorstore_path):\n",
    "        logging.error(f\"FAISS vectorstore path does not exist or is not a directory: {current_vectorstore_path}\"); state[\"vectorstore\"] = None\n",
    "    else:\n",
    "        try:\n",
    "            logging.info(f\"Loading FAISS vectorstore from '{current_vectorstore_path}' ...\")\n",
    "            loaded_vectorstore = FAISS.load_local(\n",
    "                folder_path=current_vectorstore_path, embeddings=embeddings, allow_dangerous_deserialization=True\n",
    "            )\n",
    "            logging.info(f\"FAISS vectorstore loaded with {loaded_vectorstore.index.ntotal} embeddings.\")\n",
    "            state = {**state, \"vectorstore\": loaded_vectorstore}\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Failed to load FAISS vectorstore: {e}\"); state[\"vectorstore\"] = None\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state\n",
    "\n",
    "def instantiate_retriever(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"instantiate_retriever_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} ---\")\n",
    "    if \"retriever\" not in state: state[\"retriever\"] = None\n",
    "    vectorstore = state.get(\"vectorstore\")\n",
    "    if vectorstore is None: logging.error(\"Vectorstore not loaded.\"); state[\"retriever\"] = None\n",
    "    else:\n",
    "        search_kwargs = {\"k\": k_chunks_retriever}\n",
    "        try:\n",
    "            base_retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)\n",
    "            logging.info(f\"Base retriever instantiated with default k={k_chunks_retriever}.\")\n",
    "            state = {**state, \"retriever\": base_retriever}\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Failed to instantiate base retriever: {e}\"); state[\"retriever\"] = None\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state\n",
    "\n",
    "async def _async_retrieve_docs_with_embeddings_for_file(\n",
    "    vectorstore: FAISS, file_name: str, query_embeddings_list: List[List[float]],\n",
    "    query_list_texts: List[str], k_per_query: int\n",
    ") -> tuple[str, Optional[List[Document]]]:\n",
    "    logging.info(f\"Retrieving for file: {file_name} using {len(query_embeddings_list)} pre-computed query embeddings.\")\n",
    "    retrieved_docs: List[Document] = []\n",
    "    try:\n",
    "        for i, query_embedding in enumerate(query_embeddings_list):\n",
    "            docs_for_embedding = await vectorstore.asimilarity_search_by_vector(\n",
    "                embedding=query_embedding, k=k_per_query, filter={\"file\": file_name}\n",
    "            )\n",
    "            retrieved_docs.extend(docs_for_embedding)\n",
    "        unique_docs_map: Dict[tuple, Document] = {}\n",
    "        for doc in retrieved_docs:\n",
    "            key = (doc.metadata.get(\"file\"), doc.metadata.get(\"page\"), doc.page_content.strip() if hasattr(doc, 'page_content') else \"\")\n",
    "            if key not in unique_docs_map: unique_docs_map[key] = doc\n",
    "        final_unique_docs = list(unique_docs_map.values())\n",
    "        logging.info(f\"[{file_name}] Retrieved {len(retrieved_docs)} raw -> {len(final_unique_docs)} unique docs.\")\n",
    "        return file_name, final_unique_docs\n",
    "    except Exception as e_retrieve:\n",
    "        logging.exception(f\"[{file_name}] Error during similarity search by vector: {e_retrieve}\")\n",
    "        return file_name, None\n",
    "\n",
    "async def extract_documents_parallel_node(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"extract_documents_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} (Async) ---\")\n",
    "    question, llm, base_retriever, vectorstore, embeddings_model, allowed_files = (\n",
    "        state.get(\"question\"), state.get(\"llm_large\"), state.get(\"retriever\"),\n",
    "        state.get(\"vectorstore\"), state.get(\"embeddings\"), state.get(\"allowed_files\")\n",
    "    )\n",
    "    n_alternatives, k_per_query = state.get(\"n_alternatives\", 4), state.get(\"k_per_query\", k_chunks_retriever)\n",
    "    current_documents_by_file: Dict[str, List[Document]] = {}\n",
    "\n",
    "    if not question: logging.info(f\"[{node_name}] No question. Skipping extraction.\"); return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "    if not allowed_files: logging.info(f\"[{node_name}] No files selected. Skipping extraction.\"); return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "    if not all([llm, base_retriever, vectorstore, embeddings_model]):\n",
    "        logging.error(f\"[{node_name}] Missing components for extraction. Halting.\"); return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "\n",
    "    query_list: List[str] = [question] # Start with the original question\n",
    "    try:\n",
    "        logging.info(f\"[{node_name}] Generating alternative queries...\")\n",
    "        mqr_llm_chain = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm).llm_chain\n",
    "        llm_response = await mqr_llm_chain.ainvoke({\"question\": question})\n",
    "        raw_queries_text = \"\"\n",
    "        if isinstance(llm_response, dict): raw_queries_text = str(llm_response.get(mqr_llm_chain.output_key, \"\"))\n",
    "        elif isinstance(llm_response, str): raw_queries_text = llm_response\n",
    "        elif isinstance(llm_response, list): raw_queries_text = \"\\n\".join(str(item).strip() for item in llm_response if isinstance(item, str) and str(item).strip())\n",
    "        else: raw_queries_text = str(llm_response)\n",
    "        \n",
    "        alt_queries = [q.strip() for q in raw_queries_text.split(\"\\n\") if q.strip()]\n",
    "        query_list.extend(list(dict.fromkeys(alt_queries))[:n_alternatives])\n",
    "        logging.info(f\"[{node_name}] Generated {len(query_list)} total unique queries.\")\n",
    "    except Exception as e_query_gen: logging.exception(f\"[{node_name}] Failed to generate alt queries: {e_query_gen}\")\n",
    "\n",
    "    query_embeddings_list: List[List[float]] = []\n",
    "    try:\n",
    "        logging.info(f\"[{node_name}] Embedding {len(query_list)} queries...\")\n",
    "        query_embeddings_list = await embeddings_model.aembed_documents(query_list) # type: ignore\n",
    "    except Exception as e_embed: logging.exception(f\"[{node_name}] Failed to embed queries: {e_embed}\"); return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "    if not query_embeddings_list or len(query_embeddings_list) != len(query_list):\n",
    "        logging.error(f\"[{node_name}] Query embedding failed/mismatched.\"); return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "\n",
    "    tasks = [\n",
    "        _async_retrieve_docs_with_embeddings_for_file(\n",
    "            vectorstore, f_name, query_embeddings_list, query_list, k_per_query # type: ignore\n",
    "        ) for f_name in allowed_files\n",
    "    ]\n",
    "    if tasks:\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for f_name, docs in results: current_documents_by_file[f_name] = docs if docs else []\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return {**state, \"documents_by_file\": current_documents_by_file}\n",
    "\n",
    "async def generate_individual_answers_node(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"generate_answers_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} (Async) ---\")\n",
    "    \n",
    "    question = state.get(\"question\")\n",
    "    documents_by_file = state.get(\"documents_by_file\")\n",
    "    # Correctly handle None for allowed_files\n",
    "    _allowed_files = state.get(\"allowed_files\")\n",
    "    initial_files_for_answers = _allowed_files if _allowed_files is not None else []\n",
    "\n",
    "    current_individual_answers: Dict[str, str] = {\n",
    "        filename: f\"No relevant information found in '{filename}' for this question.\" for filename in initial_files_for_answers\n",
    "    }\n",
    "    # Ensure 'generation' is preserved or initialized if not present\n",
    "    current_generation = state.get(\"generation\")\n",
    "    state_to_return = {**state, \"individual_answers\": current_individual_answers, \"generation\": current_generation}\n",
    "\n",
    "\n",
    "    if not question: \n",
    "        logging.info(f\"[{node_name}] No question. Skipping individual answer generation.\")\n",
    "    elif not documents_by_file: \n",
    "        logging.info(f\"[{node_name}] No 'documents_by_file' from previous step. Using default 'no info' messages for allowed files.\")\n",
    "    else:\n",
    "        logging.info(f\"[{node_name}] Attempting to generate answers for files with documents: {list(f for f, d in documents_by_file.items() if d)}\")\n",
    "        prompt_text = \"\"\"You are an expert assistant. Answer the user's question based ONLY on the provided context from a SINGLE FILE.\n",
    "Context from File '{filename}' (Chunks from Pages X, Y, Z...):\n",
    "{context}\n",
    "Question: {question}\n",
    "Detailed Answer (with citations like \"quote...\" ({filename}, Page X)):\"\"\"\n",
    "        prompt_template = PromptTemplate(template=prompt_text, input_variables=[\"context\", \"question\", \"filename\"])\n",
    "        llm = AzureChatOpenAI(temperature=0.1, api_key=api_key, openai_api_version=openai_api_version, azure_deployment=\"gpt-4o\", azure_endpoint=azure_endpoint, max_tokens=2000)\n",
    "        chain = prompt_template | llm | StrOutputParser()\n",
    "        \n",
    "        async def _gen_ans(fname: str, fdocs: List[Document], q: str) -> tuple[str, str]:\n",
    "            if not fdocs: return fname, f\"No relevant documents found in '{fname}' to answer the question.\"\n",
    "            # Ensure filename in context string is the actual filename for the prompt\n",
    "            ctx = \"\\n\\n\".join([f\"--- Context from Page {d.metadata.get('page', 'N/A')} (File: {fname}) ---\\n{d.page_content}\" for d in fdocs])\n",
    "            try: return fname, await chain.ainvoke({\"context\": ctx, \"question\": q, \"filename\": fname})\n",
    "            except Exception as e: logging.exception(f\"Error for {fname}: {e}\"); return fname, f\"An error occurred while generating the answer for file '{fname}': {e}\"\n",
    "        \n",
    "        tasks = []\n",
    "        # Iterate through initial_files_for_answers to ensure we only process allowed files\n",
    "        # And check if documents_by_file has entries for them\n",
    "        for fname_allowed in initial_files_for_answers:\n",
    "            if fname_allowed in documents_by_file and documents_by_file[fname_allowed]:\n",
    "                tasks.append(_gen_ans(fname_allowed, documents_by_file[fname_allowed], question))\n",
    "            else:\n",
    "                # This file was allowed, but no docs were extracted for it or docs_by_file is missing it.\n",
    "                # The default message in current_individual_answers for this file is already set.\n",
    "                logging.info(f\"File '{fname_allowed}' either had no extracted documents or was not in documents_by_file. Retaining default 'no info' message.\")\n",
    "\n",
    "        if tasks:\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for res in results:\n",
    "                if isinstance(res, tuple) and len(res) == 2: \n",
    "                    current_individual_answers[res[0]] = res[1]\n",
    "                elif isinstance(res, Exception): # Handle direct exceptions from gather\n",
    "                    logging.error(f\"A task in answer generation failed with an exception: {res}\")\n",
    "                    # Potentially try to find which task failed if needed, though complex here\n",
    "                else: \n",
    "                    logging.error(f\"Unexpected task result in answer gen: {res}\")\n",
    "        else:\n",
    "            logging.info(f\"[{node_name}] No tasks created for answer generation (e.g., no allowed files had relevant documents).\")\n",
    "        state_to_return[\"individual_answers\"] = current_individual_answers # Update with results or defaults\n",
    "    \n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state_to_return\n",
    "\n",
    "def format_raw_documents_for_synthesis_node(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"format_raw_documents_for_synthesis_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} ---\")\n",
    "    \n",
    "    documents_by_file = state.get(\"documents_by_file\")\n",
    "    allowed_files = state.get(\"allowed_files\") if state.get(\"allowed_files\") is not None else []\n",
    "    formatted_raw_docs = \"\"\n",
    "\n",
    "    if documents_by_file:\n",
    "        for filename in allowed_files: # Iterate based on allowed_files to maintain order and selection\n",
    "            docs_list = documents_by_file.get(filename)\n",
    "            if docs_list: # Check if docs exist for this allowed file\n",
    "                formatted_raw_docs += f\"--- Start of Context from File: {filename} ---\\n\\n\"\n",
    "                for doc in docs_list:\n",
    "                    page = doc.metadata.get('page', 'N/A')\n",
    "                    formatted_raw_docs += f\"Page {page}:\\n{doc.page_content}\\n\\n---\\n\\n\"\n",
    "                formatted_raw_docs += f\"--- End of Context from File: {filename} ---\\n\\n\"\n",
    "            else:\n",
    "                # If an allowed file has no documents in documents_by_file (e.g. extraction yielded nothing for it)\n",
    "                formatted_raw_docs += f\"--- No Content Extracted for File: {filename} ---\\n\\n\"\n",
    "\n",
    "    if not formatted_raw_docs and allowed_files: # If there were allowed files but nothing got formatted\n",
    "        formatted_raw_docs = \"No documents were retrieved or formatted for the selected files and question.\"\n",
    "    elif not allowed_files:\n",
    "        formatted_raw_docs = \"No files were selected for processing.\"\n",
    "\n",
    "\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return {**state, \"raw_documents_for_synthesis\": formatted_raw_docs.strip()}\n",
    "\n",
    "def _format_conversation_history(history: Optional[List[Dict[str, str]]]) -> str:\n",
    "    if not history: return \"No previous conversation history.\"\n",
    "    return \"\\n\\n\".join([f\"User: {t.get('user_question', 'N/A')}\\nAssistant: {t.get('assistant_response', 'N/A')}\" for t in history])\n",
    "\n",
    "async def _async_combine_answer_chunk(\n",
    "    question: str, answer_chunk_input: Union[Dict[str, str], str], llm_combiner: BaseLanguageModel,\n",
    "    combination_prompt_template: PromptTemplate, chunk_name: str, conversation_history_str: str,\n",
    "    is_raw_chunk: bool # This parameter helps select the right content formatting for the prompt\n",
    ") -> str:\n",
    "    logging.info(f\"Combining answer chunk: {chunk_name} (is_raw_chunk: {is_raw_chunk}).\")\n",
    "    formatted_chunk_content_for_prompt: str\n",
    "    \n",
    "    if is_raw_chunk and isinstance(answer_chunk_input, str):\n",
    "        # Input is already a pre-formatted string of raw documents for this chunk\n",
    "        formatted_chunk_content_for_prompt = answer_chunk_input\n",
    "    elif not is_raw_chunk and isinstance(answer_chunk_input, dict):\n",
    "        # Input is a dict of filename: processed_answer\n",
    "        temp_content = \"\"\n",
    "        for filename, answer in answer_chunk_input.items():\n",
    "            temp_content += f\"--- Answer based on file: {filename} ---\\n{answer}\\n\\n\"\n",
    "        formatted_chunk_content_for_prompt = temp_content.strip()\n",
    "    else:\n",
    "        logging.error(f\"Invalid answer_chunk_input type for chunk {chunk_name}\")\n",
    "        return f\"Error: Invalid input for combining chunk {chunk_name}.\"\n",
    "\n",
    "    combination_chain = combination_prompt_template | llm_combiner | StrOutputParser()\n",
    "    try:\n",
    "        # For intermediate chunks, files_no_info and files_errors are specific to this chunk's content\n",
    "        # or passed as \"None available...\" if not applicable at this stage.\n",
    "        no_info_for_chunk_prompt = \"Not applicable for this intermediate chunk.\"\n",
    "        errors_for_chunk_prompt = \"Not applicable for this intermediate chunk.\"\n",
    "\n",
    "        combined_text = await combination_chain.ainvoke({\n",
    "            \"question\": question,\n",
    "            \"formatted_answers_or_raw_docs\": formatted_chunk_content_for_prompt, # Use the correctly formatted content\n",
    "            \"files_no_info\": no_info_for_chunk_prompt, \n",
    "            \"files_errors\": errors_for_chunk_prompt,\n",
    "            \"conversation_history\": conversation_history_str \n",
    "        })\n",
    "        logging.info(f\"Successfully combined answer chunk: {chunk_name}.\")\n",
    "        return combined_text\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error combining answer chunk {chunk_name}: {e}\")\n",
    "        # Fallback returns the formatted content that failed to combine\n",
    "        return f\"Error combining chunk {chunk_name}. Raw content for this chunk:\\n\" + formatted_chunk_content_for_prompt\n",
    "\n",
    "\n",
    "async def combine_answers_node(state: GraphState) -> GraphState:\n",
    "    t_node_start = time.perf_counter()\n",
    "    node_name = \"combine_answers_node\"\n",
    "    logging.info(f\"--- Starting Node: {node_name} (Async) ---\")\n",
    "\n",
    "    question = state.get(\"question\")\n",
    "    individual_answers = state.get(\"individual_answers\")\n",
    "    allowed_files = state.get(\"allowed_files\")\n",
    "    conversation_history = state.get(\"conversation_history\")\n",
    "    bypass_flag = state.get(\"bypass_individual_generation\", False)\n",
    "    raw_docs_for_synthesis = state.get(\"raw_documents_for_synthesis\")\n",
    "    \n",
    "    output_generation: Optional[str] = \"An error occurred during response synthesis.\"\n",
    "    state_to_return = {**state} # Start with current state\n",
    "\n",
    "    if not allowed_files:\n",
    "        output_generation = \"Please select one or more files to analyze.\"\n",
    "    elif not question:\n",
    "        # Ensure allowed_files is not None before join\n",
    "        files_str = ', '.join(allowed_files) if allowed_files else \"any files\"\n",
    "        output_generation = f\"Files selected: {files_str}. Please ask a question.\"\n",
    "    else: # Files are selected and a question is asked\n",
    "        conversation_history_str = _format_conversation_history(conversation_history)\n",
    "        llm_combine_instance = AzureChatOpenAI(\n",
    "            temperature=0.0, api_key=api_key, openai_api_version=openai_api_version,\n",
    "            azure_deployment=\"gpt-4o\", azure_endpoint=azure_endpoint,\n",
    "        )\n",
    "\n",
    "        prompt_template_combine_processed_text = \"\"\"You are an expert synthesis assistant. Your task is to combine multiple PRE-PROCESSED answers, each generated from a different source file in response to the same user question. Create a single, comprehensive, and well-structured response.\n",
    "Conversation History (if any):\n",
    "{conversation_history}\n",
    "User's CURRENT Question: {question}\n",
    "Individual PRE-PROCESSED Answers from Different Files:\n",
    "{formatted_answers_or_raw_docs}\n",
    "Files Processed with No Relevant Info Found (for current question): {files_no_info}\n",
    "Files Processed with Errors (for current question): {files_errors}\n",
    "Instructions: Synthesize these pre-processed answers. Preserve all details and original citations (e.g., \"quote...\" (filename.pdf, Page X)). Attribute clearly. Structure logically. Handle contradictions. Acknowledge files noted as having no info or errors.\n",
    "Synthesized Comprehensive Answer:\"\"\"\n",
    "\n",
    "        prompt_template_combine_raw_text = \"\"\"You are an expert AI assistant. Your task is to answer the user's CURRENT question comprehensively based ONLY on the provided RAW text chunks extracted from one or more files.\n",
    "Conversation History (if any):\n",
    "{conversation_history}\n",
    "User's CURRENT Question: {question}\n",
    "RAW Text Chunks Extracted from Files (File and Page numbers are indicated within the text):\n",
    "{formatted_answers_or_raw_docs}\n",
    "Files Processed with No Relevant Info Found (meaning no chunks were extracted for them for the current question): {files_no_info}\n",
    "Files Processed with Errors (during extraction for the current question): {files_errors}\n",
    "Instructions: Thoroughly read all provided raw text chunks. Answer the user's question directly and ONLY based on this raw text. Identify the most relevant information. Include direct quotes with filename and page citations (e.g., \"quote...\" (filename.pdf, Page X)). If information is not found, state that. Structure logically.\n",
    "Synthesized Comprehensive Answer from RAW Documents:\"\"\"\n",
    "\n",
    "        active_prompt_template_text = prompt_template_combine_raw_text if bypass_flag else prompt_template_combine_processed_text\n",
    "        \n",
    "        combination_prompt = PromptTemplate(\n",
    "            template=active_prompt_template_text,\n",
    "            input_variables=[\"question\", \"formatted_answers_or_raw_docs\", \"files_no_info\", \"files_errors\", \"conversation_history\"]\n",
    "        )\n",
    "\n",
    "        files_with_no_info_final_list: List[str] = []\n",
    "        files_with_errors_final_list: List[str] = []\n",
    "        \n",
    "        content_for_llm: str = \"\" # This will hold either formatted individual answers or raw_docs_for_synthesis\n",
    "\n",
    "        if bypass_flag:\n",
    "            logging.info(f\"[{node_name}] Bypassing individual answers. Combining raw documents.\")\n",
    "            content_for_llm = raw_docs_for_synthesis if raw_docs_for_synthesis else \"No raw documents available for synthesis.\"\n",
    "            if not raw_docs_for_synthesis or raw_docs_for_synthesis == \"No documents were retrieved for the selected files and question.\" or raw_docs_for_synthesis == \"No files were selected for processing.\":\n",
    "                output_generation = raw_docs_for_synthesis # Use the message from formatting node\n",
    "                if conversation_history: output_generation += \"\\nIs there anything else I can help you with?\"\n",
    "            else:\n",
    "                # Determine files_with_no_info_list for raw path\n",
    "                docs_by_file = state.get(\"documents_by_file\", {})\n",
    "                if allowed_files: # Ensure allowed_files is not None\n",
    "                    for af in allowed_files:\n",
    "                        if not docs_by_file.get(af): \n",
    "                            files_with_no_info_final_list.append(f\"`{af}`\")\n",
    "                # Error tracking for raw path is simplified for now\n",
    "                files_with_errors_final_list.append(\"Error tracking during raw document extraction is not detailed at this stage.\")\n",
    "        \n",
    "        else: # Standard path: combine pre-processed individual_answers\n",
    "            if not individual_answers:\n",
    "                 output_generation = \"No individual answers available to combine.\"\n",
    "            else:\n",
    "                answers_to_combine: Dict[str, str] = {}\n",
    "                for filename, answer in individual_answers.items():\n",
    "                    if \"An error occurred\" in answer: files_with_errors_final_list.append(f\"`{filename}`\")\n",
    "                    elif \"No relevant information found\" in answer or \"No relevant documents were found\" in answer:\n",
    "                        files_with_no_info_final_list.append(f\"`{filename}`\")\n",
    "                    else: answers_to_combine[filename] = answer\n",
    "\n",
    "                if not answers_to_combine:\n",
    "                    msg_parts = [f\"I couldn't find specific information to answer: '{question}'.\"]\n",
    "                    if files_with_no_info_final_list: msg_parts.append(f\"No info in: {', '.join(files_with_no_info_final_list)}.\")\n",
    "                    if files_with_errors_final_list: msg_parts.append(f\"Errors for: {', '.join(files_with_errors_final_list)}.\")\n",
    "                    output_generation = \"\\n\".join(msg_parts)\n",
    "                else: # We have substantive answers to combine\n",
    "                    if len(answers_to_combine) <= COMBINE_THRESHOLD:\n",
    "                        temp_fmt_ans = \"\"\n",
    "                        for fname, ans in answers_to_combine.items(): temp_fmt_ans += f\"--- Answer from file: {fname} ---\\n{ans}\\n\\n\"\n",
    "                        content_for_llm = temp_fmt_ans.strip()\n",
    "                    else: # Two-stage for processed answers\n",
    "                        answer_items = list(answers_to_combine.items())\n",
    "                        tasks_s1 = []\n",
    "                        for i in range(0, len(answer_items), COMBINE_THRESHOLD):\n",
    "                            chunk_dict = dict(answer_items[i:i + COMBINE_THRESHOLD])\n",
    "                            tasks_s1.append(\n",
    "                                _async_combine_answer_chunk(\n",
    "                                    question, chunk_dict, llm_combine_instance, combination_prompt,\n",
    "                                    f\"Processed Chunk {i//COMBINE_THRESHOLD + 1}\", conversation_history_str, False\n",
    "                                )\n",
    "                            )\n",
    "                        intermediate_results = await asyncio.gather(*tasks_s1, return_exceptions=True)\n",
    "                        temp_fmt_s2 = \"\"\n",
    "                        valid_interm_texts = []\n",
    "                        for i_res, res in enumerate(intermediate_results):\n",
    "                            if isinstance(res, str): temp_fmt_s2 += f\"--- Synthesized Batch {i_res+1} ---\\n{res}\\n\\n\"; valid_interm_texts.append(res)\n",
    "                            else: logging.error(f\"Error combining processed chunk {i_res+1}: {res}\"); temp_fmt_s2 += f\"--- Error Batch {i_res+1} ---\\nNot combined.\\n\\n\"\n",
    "                        if not valid_interm_texts: output_generation = \"Failed to combine intermediate chunks of answers.\"\n",
    "                        else: content_for_llm = temp_fmt_s2.strip()\n",
    "        \n",
    "        # Final LLM call if content_for_llm was prepared\n",
    "        if content_for_llm and (output_generation == \"An error occurred during response synthesis.\" or answers_to_combine): # Check if we should proceed\n",
    "            try:\n",
    "                final_chain = combination_prompt | llm_combine_instance | StrOutputParser()\n",
    "                output_generation = await final_chain.ainvoke({\n",
    "                    \"question\": question,\n",
    "                    \"formatted_answers_or_raw_docs\": content_for_llm,\n",
    "                    \"files_no_info\": \", \".join(files_with_no_info_final_list) if files_with_no_info_final_list else \"None\",\n",
    "                    \"files_errors\": \", \".join(files_with_errors_final_list) if files_with_errors_final_list else \"None\",\n",
    "                    \"conversation_history\": conversation_history_str\n",
    "                })\n",
    "                logging.info(f\"[{node_name}] Final answer generated successfully.\")\n",
    "            except Exception as e:\n",
    "                logging.exception(f\"[{node_name}] Error in final combination LLM call: {e}\")\n",
    "                output_generation = f\"Error in final synthesis: {e}. Partial/raw content: {content_for_llm[:500]}...\"\n",
    "        \n",
    "    state_to_return[\"generation\"] = output_generation\n",
    "    duration_node = time.perf_counter() - t_node_start\n",
    "    logging.info(f\"--- Node: {node_name} finished in {duration_node:.4f} seconds ---\")\n",
    "    return state_to_return\n",
    "\n",
    "\n",
    "def decide_processing_path_after_extraction(state: GraphState) -> str:\n",
    "    \"\"\"Determines the next step after document extraction.\"\"\"\n",
    "    node_name = \"decide_processing_path_after_extraction\"\n",
    "    bypass = state.get(\"bypass_individual_generation\", False)\n",
    "    question = state.get(\"question\")\n",
    "    allowed_files = state.get(\"allowed_files\")\n",
    "\n",
    "    if not question or not allowed_files:\n",
    "        logging.info(f\"[{node_name}] Decision: No question or no files selected. Routing to standard generation path for appropriate messaging.\")\n",
    "        return \"to_generate_individual_answers\" # This path will lead to combine_answers_node which handles these states\n",
    "\n",
    "    if bypass:\n",
    "        logging.info(f\"[{node_name}] Decision: Bypass individual generation is TRUE. Routing to format raw documents.\")\n",
    "        return \"to_format_raw_for_synthesis\"\n",
    "    else:\n",
    "        logging.info(f\"[{node_name}] Decision: Bypass individual generation is FALSE. Routing to generate individual answers.\")\n",
    "        return \"to_generate_individual_answers\"\n",
    "\n",
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"instantiate_embeddings_node\", instantiate_embeddings)\n",
    "workflow.add_node(\"instantiate_llm_node\", instantiate_llm_large)\n",
    "workflow.add_node(\"load_vectorstore_node\", load_faiss_vectorstore)\n",
    "workflow.add_node(\"instantiate_retriever_node\", instantiate_retriever)\n",
    "workflow.add_node(\"extract_documents_node\", extract_documents_parallel_node)\n",
    "workflow.add_node(\"format_raw_documents_node\", format_raw_documents_for_synthesis_node) \n",
    "workflow.add_node(\"generate_answers_node\", generate_individual_answers_node) \n",
    "workflow.add_node(\"combine_answers_node\", combine_answers_node) \n",
    "\n",
    "# Define the graph's flow (edges)\n",
    "workflow.set_entry_point(\"instantiate_embeddings_node\")\n",
    "workflow.add_edge(\"instantiate_embeddings_node\", \"instantiate_llm_node\")\n",
    "workflow.add_edge(\"instantiate_llm_node\", \"load_vectorstore_node\")\n",
    "workflow.add_edge(\"load_vectorstore_node\", \"instantiate_retriever_node\")\n",
    "workflow.add_edge(\"instantiate_retriever_node\", \"extract_documents_node\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"extract_documents_node\",\n",
    "    decide_processing_path_after_extraction,\n",
    "    {\n",
    "        \"to_format_raw_for_synthesis\": \"format_raw_documents_node\",\n",
    "        \"to_generate_individual_answers\": \"generate_answers_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"format_raw_documents_node\", \"combine_answers_node\")\n",
    "workflow.add_edge(\"generate_answers_node\", \"combine_answers_node\") \n",
    "workflow.add_edge(\"combine_answers_node\", END) \n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    if not all([api_key, azure_endpoint]): logging.error(\"Azure creds not found.\"); return\n",
    "\n",
    "    async def run_turn(turn_name: str, current_state: GraphState, update_history: bool = True) -> GraphState:\n",
    "        logging.info(f\"\\n--- SIMULATING {turn_name} ---\")\n",
    "        logging.info(f\"Input State - Question: {current_state.get('question')}, Allowed Files: {current_state.get('allowed_files')}, Bypass: {current_state.get('bypass_individual_generation')}\")\n",
    "        start_time = time.perf_counter()\n",
    "        # Ensure all keys from GraphState are present in current_state before invoking\n",
    "        for key in GraphState.__annotations__.keys():\n",
    "            if key not in current_state:\n",
    "                current_state[key] = None # type: ignore\n",
    "        \n",
    "        next_state = await app.ainvoke(current_state)\n",
    "        duration = time.perf_counter() - start_time\n",
    "        logging.info(f\"{turn_name} completed in {duration:.4f}s. Assistant: {next_state.get('generation')}\")\n",
    "        if update_history and next_state.get(\"question\") and next_state.get(\"generation\"):\n",
    "            hist = next_state.get(\"conversation_history\") \n",
    "            if hist is None: hist = [] # Ensure hist is a list\n",
    "            \n",
    "            hist.append({\"user_question\": next_state[\"question\"], \"assistant_response\": next_state[\"generation\"]}) \n",
    "            next_state[\"conversation_history\"] = hist[-MAX_CONVERSATION_TURNS:]\n",
    "            logging.info(f\"History updated. Length: {len(next_state['conversation_history'])}\") \n",
    "        return next_state\n",
    "\n",
    "    # Turn 0: Initial Load\n",
    "    session_state = await run_turn(\"TURN 0: Initial Load\", session_state, update_history=False)\n",
    "    \n",
    "    # Turn 1: User selects files\n",
    "    session_state[\"allowed_files\"] = ['ID_OR_Idaho_Power_2022.pdf', 'OR_City_of_Bandon_2024.pdf']\n",
    "    session_state = await run_turn(\"TURN 1: Files Selected, No Question\", session_state, update_history=False)\n",
    "\n",
    "    # Turn 2: First question (standard path)\n",
    "    session_state[\"question\"] = \"What are the current practices for vegetation management?\"\n",
    "    session_state[\"bypass_individual_generation\"] = True\n",
    "    session_state = await run_turn(\"TURN 2: First Question (Standard Path)\", session_state)\n",
    "\n",
    "    # # Turn 3: Follow-up (standard path)\n",
    "    # session_state[\"question\"] = \"What about the costs in Idaho Power's report?\"\n",
    "    # session_state[\"bypass_individual_generation\"] = True\n",
    "    # session_state = await run_turn(\"TURN 3: Follow-up (Standard Path)\", session_state)\n",
    "\n",
    "    # # Turn 4: New question (bypass path)\n",
    "    # session_state[\"question\"] = \"Summarize key safety protocols mentioned across all selected documents.\"\n",
    "    # session_state[\"bypass_individual_generation\"] = True\n",
    "    # session_state = await run_turn(\"TURN 4: New Question (Bypass Path)\", session_state)\n",
    "    \n",
    "    # # Turn 5: Follow-up on bypass path output\n",
    "    # session_state[\"question\"] = \"Regarding those safety protocols, are there specific training requirements mentioned?\"\n",
    "    # session_state[\"bypass_individual_generation\"] = True \n",
    "    # session_state = await run_turn(\"TURN 5: Follow-up on Bypass (Bypass Path)\", session_state)\n",
    "\n",
    "\n",
    "    # logging.info(\"\\n--- Final State Overview after simulated turns ---\")\n",
    "    # if session_state.get(\"conversation_history\"):\n",
    "    #     logging.info(\"  --- Final Conversation History ---\")\n",
    "    #     for i, turn_data in enumerate(session_state.get(\"conversation_history\", [])): \n",
    "    #         logging.info(f\"    Turn {i+1}: User: {turn_data['user_question']} | Assistant: {turn_data['assistant_response'][:100]}...\")\n",
    "\n",
    "    return session_state\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if not os.path.exists(os.path.join(root_directory, \".env\")):\n",
    "#         logging.warning(f\".env file not found at {os.path.join(root_directory, '.env')}.\")\n",
    "#     asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73f3cfb2-93de-4766-a195-d6307a1fdfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:40:45,966 - root - INFO - \n",
      "--- SIMULATING TURN 0: Initial Load ---\n",
      "2025-05-15 10:40:45,968 - root - INFO - Input State - Question: None, Allowed Files: None, Bypass: False\n",
      "2025-05-15 10:40:45,971 - root - INFO - --- Starting Node: instantiate_embeddings_node ---\n",
      "2025-05-15 10:40:45,972 - root - INFO - Instantiating embeddings model\n",
      "2025-05-15 10:40:45,995 - root - INFO - --- Node: instantiate_embeddings_node finished in 0.0233 seconds ---\n",
      "2025-05-15 10:40:45,996 - root - INFO - --- Starting Node: instantiate_llm_node (for query generation) ---\n",
      "2025-05-15 10:40:45,996 - root - INFO - Instantiating large LLM model (for query generation)\n",
      "2025-05-15 10:40:46,008 - root - INFO - --- Node: instantiate_llm_node finished in 0.0123 seconds ---\n",
      "2025-05-15 10:40:46,009 - root - INFO - --- Starting Node: load_vectorstore_node ---\n",
      "2025-05-15 10:40:46,010 - root - INFO - Loading FAISS vectorstore from '/Users/d3y010/repos/crvernon/knowai/test_faiss_store' ...\n",
      "2025-05-15 10:40:46,011 - root - INFO - FAISS vectorstore loaded with 198 embeddings.\n",
      "2025-05-15 10:40:46,011 - root - INFO - --- Node: load_vectorstore_node finished in 0.0016 seconds ---\n",
      "2025-05-15 10:40:46,012 - root - INFO - --- Starting Node: instantiate_retriever_node ---\n",
      "2025-05-15 10:40:46,013 - root - INFO - Base retriever instantiated with default k=25.\n",
      "2025-05-15 10:40:46,013 - root - INFO - --- Node: instantiate_retriever_node finished in 0.0006 seconds ---\n",
      "2025-05-15 10:40:46,014 - root - INFO - --- Starting Node: extract_documents_node (Async) ---\n",
      "2025-05-15 10:40:46,014 - root - INFO - [extract_documents_node] No question. Skipping extraction.\n",
      "2025-05-15 10:40:46,014 - root - INFO - [decide_processing_path_after_extraction] Decision: No question or no files selected. Routing to standard generation path for appropriate messaging.\n",
      "2025-05-15 10:40:46,015 - root - INFO - --- Starting Node: generate_answers_node (Async) ---\n",
      "2025-05-15 10:40:46,016 - root - INFO - [generate_answers_node] No question. Skipping individual answer generation.\n",
      "2025-05-15 10:40:46,016 - root - INFO - --- Node: generate_answers_node finished in 0.0003 seconds ---\n",
      "2025-05-15 10:40:46,016 - root - INFO - --- Starting Node: combine_answers_node (Async) ---\n",
      "2025-05-15 10:40:46,017 - root - INFO - --- Node: combine_answers_node finished in 0.0001 seconds ---\n",
      "2025-05-15 10:40:46,017 - root - INFO - TURN 0: Initial Load completed in 0.0482s. Assistant: Please select one or more files to analyze.\n",
      "2025-05-15 10:40:46,017 - root - INFO - \n",
      "--- SIMULATING TURN 1: Files Selected, No Question ---\n",
      "2025-05-15 10:40:46,018 - root - INFO - Input State - Question: None, Allowed Files: ['ID_OR_Idaho_Power_2022.pdf', 'OR_City_of_Bandon_2024.pdf'], Bypass: False\n",
      "2025-05-15 10:40:46,018 - root - INFO - --- Starting Node: instantiate_embeddings_node ---\n",
      "2025-05-15 10:40:46,018 - root - INFO - Using pre-instantiated embeddings model\n",
      "2025-05-15 10:40:46,019 - root - INFO - --- Node: instantiate_embeddings_node finished in 0.0004 seconds ---\n",
      "2025-05-15 10:40:46,020 - root - INFO - --- Starting Node: instantiate_llm_node (for query generation) ---\n",
      "2025-05-15 10:40:46,020 - root - INFO - Using pre-instantiated large LLM model (for query generation)\n",
      "2025-05-15 10:40:46,020 - root - INFO - --- Node: instantiate_llm_node finished in 0.0003 seconds ---\n",
      "2025-05-15 10:40:46,021 - root - INFO - --- Starting Node: load_vectorstore_node ---\n",
      "2025-05-15 10:40:46,021 - root - INFO - Vectorstore already exists in state.\n",
      "2025-05-15 10:40:46,021 - root - INFO - --- Node: load_vectorstore_node finished in 0.0004 seconds ---\n",
      "2025-05-15 10:40:46,022 - root - INFO - --- Starting Node: instantiate_retriever_node ---\n",
      "2025-05-15 10:40:46,022 - root - INFO - Base retriever instantiated with default k=25.\n",
      "2025-05-15 10:40:46,022 - root - INFO - --- Node: instantiate_retriever_node finished in 0.0004 seconds ---\n",
      "2025-05-15 10:40:46,023 - root - INFO - --- Starting Node: extract_documents_node (Async) ---\n",
      "2025-05-15 10:40:46,023 - root - INFO - [extract_documents_node] No question. Skipping extraction.\n",
      "2025-05-15 10:40:46,024 - root - INFO - [decide_processing_path_after_extraction] Decision: No question or no files selected. Routing to standard generation path for appropriate messaging.\n",
      "2025-05-15 10:40:46,025 - root - INFO - --- Starting Node: generate_answers_node (Async) ---\n",
      "2025-05-15 10:40:46,026 - root - INFO - [generate_answers_node] No question. Skipping individual answer generation.\n",
      "2025-05-15 10:40:46,026 - root - INFO - --- Node: generate_answers_node finished in 0.0004 seconds ---\n",
      "2025-05-15 10:40:46,026 - root - INFO - --- Starting Node: combine_answers_node (Async) ---\n",
      "2025-05-15 10:40:46,027 - root - INFO - --- Node: combine_answers_node finished in 0.0002 seconds ---\n",
      "2025-05-15 10:40:46,027 - root - INFO - TURN 1: Files Selected, No Question completed in 0.0096s. Assistant: Files selected: ID_OR_Idaho_Power_2022.pdf, OR_City_of_Bandon_2024.pdf. Please ask a question.\n",
      "2025-05-15 10:40:46,028 - root - INFO - \n",
      "--- SIMULATING TURN 2: First Question (Standard Path) ---\n",
      "2025-05-15 10:40:46,028 - root - INFO - Input State - Question: What are the current practices for vegetation management?, Allowed Files: ['ID_OR_Idaho_Power_2022.pdf', 'OR_City_of_Bandon_2024.pdf'], Bypass: True\n",
      "2025-05-15 10:40:46,028 - root - INFO - --- Starting Node: instantiate_embeddings_node ---\n",
      "2025-05-15 10:40:46,029 - root - INFO - Using pre-instantiated embeddings model\n",
      "2025-05-15 10:40:46,029 - root - INFO - --- Node: instantiate_embeddings_node finished in 0.0004 seconds ---\n",
      "2025-05-15 10:40:46,030 - root - INFO - --- Starting Node: instantiate_llm_node (for query generation) ---\n",
      "2025-05-15 10:40:46,031 - root - INFO - Using pre-instantiated large LLM model (for query generation)\n",
      "2025-05-15 10:40:46,031 - root - INFO - --- Node: instantiate_llm_node finished in 0.0006 seconds ---\n",
      "2025-05-15 10:40:46,032 - root - INFO - --- Starting Node: load_vectorstore_node ---\n",
      "2025-05-15 10:40:46,032 - root - INFO - Vectorstore already exists in state.\n",
      "2025-05-15 10:40:46,033 - root - INFO - --- Node: load_vectorstore_node finished in 0.0005 seconds ---\n",
      "2025-05-15 10:40:46,033 - root - INFO - --- Starting Node: instantiate_retriever_node ---\n",
      "2025-05-15 10:40:46,033 - root - INFO - Base retriever instantiated with default k=25.\n",
      "2025-05-15 10:40:46,034 - root - INFO - --- Node: instantiate_retriever_node finished in 0.0003 seconds ---\n",
      "2025-05-15 10:40:46,034 - root - INFO - --- Starting Node: extract_documents_node (Async) ---\n",
      "2025-05-15 10:40:46,035 - root - INFO - [extract_documents_node] Generating alternative queries...\n",
      "2025-05-15 10:40:47,660 - httpx - INFO - HTTP Request: POST https://openai-im3.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-05-15 10:40:47,669 - root - INFO - [extract_documents_node] Generated 4 total unique queries.\n",
      "2025-05-15 10:40:47,670 - root - INFO - [extract_documents_node] Embedding 4 queries...\n",
      "2025-05-15 10:40:48,468 - httpx - INFO - HTTP Request: POST https://openai-im3.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-05-15 10:40:48,868 - root - INFO - Retrieving for file: ID_OR_Idaho_Power_2022.pdf using 4 pre-computed query embeddings.\n",
      "2025-05-15 10:40:48,870 - root - INFO - Retrieving for file: OR_City_of_Bandon_2024.pdf using 4 pre-computed query embeddings.\n",
      "2025-05-15 10:40:48,881 - root - INFO - [OR_City_of_Bandon_2024.pdf] Retrieved 15 raw -> 4 unique docs.\n",
      "2025-05-15 10:40:48,882 - root - INFO - [ID_OR_Idaho_Power_2022.pdf] Retrieved 40 raw -> 12 unique docs.\n",
      "2025-05-15 10:40:48,886 - root - INFO - --- Node: extract_documents_node finished in 2.8518 seconds ---\n",
      "2025-05-15 10:40:48,888 - root - INFO - [decide_processing_path_after_extraction] Decision: Bypass individual generation is TRUE. Routing to format raw documents.\n",
      "2025-05-15 10:40:48,890 - root - INFO - --- Starting Node: format_raw_documents_for_synthesis_node ---\n",
      "2025-05-15 10:40:48,891 - root - INFO - --- Node: format_raw_documents_for_synthesis_node finished in 0.0006 seconds ---\n",
      "2025-05-15 10:40:48,893 - root - INFO - --- Starting Node: combine_answers_node (Async) ---\n",
      "2025-05-15 10:40:48,910 - root - INFO - [combine_answers_node] Bypassing individual answers. Combining raw documents.\n",
      "2025-05-15 10:40:55,992 - httpx - INFO - HTTP Request: POST https://openai-im3.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-05-15 10:40:56,000 - root - INFO - [combine_answers_node] Final answer generated successfully.\n",
      "2025-05-15 10:40:56,001 - root - INFO - --- Node: combine_answers_node finished in 7.1083 seconds ---\n",
      "2025-05-15 10:40:56,007 - root - INFO - TURN 2: First Question (Standard Path) completed in 9.9792s. Assistant: Current practices for vegetation management, as outlined in the provided documents, focus on maintaining safe distances between vegetation and power lines to prevent outages and reduce wildfire risks. Here are the key practices:\n",
      "\n",
      "1. **Pruning and Clearing**: Vegetation is managed through methods such as directional or natural pruning, which is recommended by the International Society of Arboriculture and the ANSI A300 standards. This involves removing dead branches, weak attachments, and trees that pose a risk to power lines (ID_OR_Idaho_Power_2022.pdf, Page 39).\n",
      "\n",
      "2. **Use of Herbicides and Growth Regulators**: When appropriate and compliant with federal and state requirements, tree-growth regulators and spot herbicide treatments are used to reduce the re-growth of deciduous shrubs and trees, thereby extending maintenance cycles (ID_OR_Idaho_Power_2022.pdf, Page 39).\n",
      "\n",
      "3. **Inspection and Monitoring**: Regular inspections are conducted to identify and mitigate vegetation hazards. This includes aerial and ground patrols to assess the threat level of vegetation near transmission and distribution lines (ID_OR_Idaho_Power_2022.pdf, Page 38).\n",
      "\n",
      "4. **Clearing Cycles**: Transmission lines are cleared on a 3-year cycle for urban and rural valley areas and a 6-year cycle for mountain areas. Distribution lines are cleared on a 3-year cycle, with mid-cycle pruning in high-risk zones (ID_OR_Idaho_Power_2022.pdf, Page 39; OR_City_of_Bandon_2024.pdf, Page 7).\n",
      "\n",
      "5. **Quality Control**: Quality assurance and control audits are conducted to ensure that vegetation management work meets the required standards. This includes 100% quality control reviews of contractor work by certified arborists (ID_OR_Idaho_Power_2022.pdf, Page 39).\n",
      "\n",
      "6. **Substation Risk Management**: Vegetation around substations is managed through spraying to minimize growth and reduce fire risk. Routine inspections and maintenance of a ground layer of rock, gravel, or concrete are also conducted to prevent combustible growth (OR_City_of_Bandon_2024.pdf, Page 7).\n",
      "\n",
      "7. **System Hardening**: Efforts are made to harden the system against wildfire risks, such as prioritizing underground power lines and doubling insulation on above-ground wires (OR_City_of_Bandon_2024.pdf, Page 7).\n",
      "\n",
      "These practices are part of comprehensive vegetation management programs aimed at ensuring public safety, maintaining electric reliability, and mitigating wildfire risks.\n",
      "2025-05-15 10:40:56,008 - root - INFO - History updated. Length: 1\n"
     ]
    }
   ],
   "source": [
    "session_state = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "786f2cf3-f03e-49ec-86dc-dc3897859d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Current practices for vegetation management, as outlined in the provided documents, focus on maintaining safe distances between vegetation and power lines to prevent outages and reduce wildfire risks. Here are the key practices:\\n\\n1. **Pruning and Clearing**: Vegetation is managed through methods such as directional or natural pruning, which is recommended by the International Society of Arboriculture and the ANSI A300 standards. This involves removing dead branches, weak attachments, and trees that pose a risk to power lines (ID_OR_Idaho_Power_2022.pdf, Page 39).\\n\\n2. **Use of Herbicides and Growth Regulators**: When appropriate and compliant with federal and state requirements, tree-growth regulators and spot herbicide treatments are used to reduce the re-growth of deciduous shrubs and trees, thereby extending maintenance cycles (ID_OR_Idaho_Power_2022.pdf, Page 39).\\n\\n3. **Inspection and Monitoring**: Regular inspections are conducted to identify and mitigate vegetation hazards. This includes aerial and ground patrols to assess the threat level of vegetation near transmission and distribution lines (ID_OR_Idaho_Power_2022.pdf, Page 38).\\n\\n4. **Clearing Cycles**: Transmission lines are cleared on a 3-year cycle for urban and rural valley areas and a 6-year cycle for mountain areas. Distribution lines are cleared on a 3-year cycle, with mid-cycle pruning in high-risk zones (ID_OR_Idaho_Power_2022.pdf, Page 39; OR_City_of_Bandon_2024.pdf, Page 7).\\n\\n5. **Quality Control**: Quality assurance and control audits are conducted to ensure that vegetation management work meets the required standards. This includes 100% quality control reviews of contractor work by certified arborists (ID_OR_Idaho_Power_2022.pdf, Page 39).\\n\\n6. **Substation Risk Management**: Vegetation around substations is managed through spraying to minimize growth and reduce fire risk. Routine inspections and maintenance of a ground layer of rock, gravel, or concrete are also conducted to prevent combustible growth (OR_City_of_Bandon_2024.pdf, Page 7).\\n\\n7. **System Hardening**: Efforts are made to harden the system against wildfire risks, such as prioritizing underground power lines and doubling insulation on above-ground wires (OR_City_of_Bandon_2024.pdf, Page 7).\\n\\nThese practices are part of comprehensive vegetation management programs aimed at ensuring public safety, maintaining electric reliability, and mitigating wildfire risks.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_state[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be5334-c874-44a4-baa0-22e355f6b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3ab98-9435-4073-99b9-9b67d24b412a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a351170-5a8d-4819-99bb-cb3fe4b6dbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a6252-37d4-49d3-8030-976f769ce349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa144686-7881-436c-bd3e-0056fb51abce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d626f30-eacb-4428-b405-46df05da4e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eacd34a0-b2ca-420b-8926-111f6fbaa21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__([<p>__start__</p>]):::first\\n\\tinstantiate_embeddings_node(instantiate_embeddings_node)\\n\\tinstantiate_llm_node(instantiate_llm_node)\\n\\tload_vectorstore_node(load_vectorstore_node)\\n\\tinstantiate_retriever_node(instantiate_retriever_node)\\n\\textract_documents_node(extract_documents_node)\\n\\tformat_raw_documents_node(format_raw_documents_node)\\n\\tgenerate_answers_node(generate_answers_node)\\n\\tcombine_answers_node(combine_answers_node)\\n\\t__end__([<p>__end__</p>]):::last\\n\\t__start__ --> instantiate_embeddings_node;\\n\\tcombine_answers_node --> __end__;\\n\\tformat_raw_documents_node --> combine_answers_node;\\n\\tgenerate_answers_node --> combine_answers_node;\\n\\tinstantiate_embeddings_node --> instantiate_llm_node;\\n\\tinstantiate_llm_node --> load_vectorstore_node;\\n\\tinstantiate_retriever_node --> extract_documents_node;\\n\\tload_vectorstore_node --> instantiate_retriever_node;\\n\\textract_documents_node -. &nbsp;to_format_raw_for_synthesis&nbsp; .-> format_raw_documents_node;\\n\\textract_documents_node -. &nbsp;to_generate_individual_answers&nbsp; .-> generate_answers_node;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65878459-329b-4937-b03c-e2932b16d907",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:468\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    464\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    465\u001b[39m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    466\u001b[39m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    467\u001b[39m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m             \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mraise_from\u001b[39m\u001b[34m(value, from_value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:463\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     httplib_response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:802\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    800\u001b[39m     e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/util/retry.py:552\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/packages/six.py:770\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m    769\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:716\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:470\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/urllib3/connectionpool.py:358\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    359\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m % timeout_value\n\u001b[32m    360\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m display(Image(\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py3.13.3_ai/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88caf700-cf64-4b54-97cc-711b07f2e2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179a125-dc0d-4c8c-9f8f-bba165eafe87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.13.3_ai",
   "language": "python",
   "name": "py3.13.3_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
