import streamlit as st
import os
from langchain_openai import ChatOpenAI, AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# --- Configuration ---
api_key = os.getenv("AZURE_OPENAI_API_KEY", default=None)
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", default=None)
# Use a default deployment name if not set, or handle error if critical
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o") # Defaulting to gpt-4o deployment name
embeddings_deployment = os.getenv("AZURE_EMBEDDINGS_DEPLOYMENT", "text-embedding-3-large") # Defaulting embeddings deployment
openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01") # Defaulting API version


# --- Streamlit App UI ---
st.set_page_config(layout="wide") # Use wide layout for more space
st.title("üìù LLM vs. Human Answer Validator")
st.write("""
Enter a question, the answer generated by a Large Language Model (LLM), and a human-written answer.
This tool will use another LLM (OpenAI's GPT model) to compare the two answers, provide a likeness score,
and highlight key differences or similarities based on the context of the question.
""")
# --- Input Fields ---
# Question input - now full width
st.subheader("Question")
question = st.text_area("Enter the question asked:", height=100, key="question_input")

# Columns for LLM and Human answers
col1, col2 = st.columns(2) # Create two columns for the answers below the question

with col1:
    st.subheader("ü§ñ LLM Answer")
    llm_answer = st.text_area("Paste the LLM's answer here:", height=300, key="llm_answer_input")

with col2:
    st.subheader("üë§ Human Answer")
    human_answer = st.text_area("Paste the human's answer here:", height=300, key="human_answer_input")

# --- Comparison Logic ---
if st.button("üîç Compare Answers", use_container_width=True):
    if question and llm_answer and human_answer:
        st.divider() # Add a visual separator

        # --- Explanation of Scoring ---
        st.subheader("üí° How the Likeness Score is Determined")
        st.info("""
        The 'Likeness Score' is **not calculated by a fixed mathematical formula** within this application.
        Instead, the question, LLM answer, and human answer are sent to an advanced external Large Language Model (OpenAI's GPT-4o).

        This external LLM is instructed to:
        1.  Analyze both answers based on the **question's context**.
        2.  Evaluate **semantic similarity** (meaning), **factual accuracy**, **completeness**, and **relevance**.
        3.  Assign a score from **0 (completely different/wrong) to 100 (semantically identical/fully captures key points)** based on its assessment compared to the human answer.
        4.  Provide a detailed **reasoning** for the score.

        The score reflects the LLM's nuanced understanding of language and the comparison criteria provided in its instructions.
        """)
        st.divider() # Add another separator before results

        st.subheader("üìä Comparison Results")

        try:
            # Initialize the ChatOpenAI model
            # You might want to specify a model_name like "gpt-4" or "gpt-3.5-turbo"
            llm = AzureChatOpenAI(
                temperature=0.2, 
                api_key=api_key, 
                openai_api_version=openai_api_version,
                azure_deployment=deployment, 
                azure_endpoint=azure_endpoint,
            )

            # Define the system message (instructions for the LLM)
            system_message_prompt = """
            You are an expert evaluator comparing answers to a specific question.
            You will be given a question, an answer from an LLM, and an answer from a Human.
            Your tasks are to:
            1.  Analyze both answers in the context of the question.
            2.  Provide a 'Likeness Score' from 0 to 100, where 100 means the LLM answer is semantically identical or captures all the key points of the human answer accurately, and 0 means it's completely different or incorrect compared to the human answer. Consider factual accuracy, completeness, and relevance to the question.
            3.  Explain the reasoning behind your assigned score.
            4.  Identify and list the key similarities and differences between the two answers. Focus on semantic meaning, information conveyed, and tone if relevant.

            Present your output clearly, starting with the Likeness Score.
            """

            # Define the human message (the user's input)
            human_message_prompt = f"""
            **Question:**
            {question}

            **LLM Answer:**
            {llm_answer}

            **Human Answer:**
            {human_answer}

            **Evaluation:**
            Please provide the Likeness Score, reasoning, similarities, and differences based on the instructions.
            """

            # Create message objects
            messages = [
                SystemMessage(content=system_message_prompt),
                HumanMessage(content=human_message_prompt)
            ]

            # Call the LLM
            with st.spinner("üß† Analyzing answers... This might take a moment."):
                response = llm.invoke(messages)
                comparison_result = response.content

            # Display the result
            st.markdown(comparison_result) # Use markdown for potentially better formatting from the LLM

        except Exception as e:
            st.error(f"An error occurred during comparison: {e}")
            st.exception(e) # Show detailed traceback

    else:
        st.warning("‚ö†Ô∏è Please fill in all fields: Question, LLM Answer, and Human Answer.")

