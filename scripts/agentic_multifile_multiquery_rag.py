import streamlit as st
import streamlit.components.v1 as components

# --- Page Config (MUST BE FIRST STREAMLIT COMMAND) ---
st.set_page_config(layout="wide")

# --- Logo Display ---
# Place logos at the top-left using columns
# Ensure the 'img' folder is in the same directory as your script
logo_col1, logo_col2, title_col = st.columns([1, 1, 8]) # Adjust ratios as needed
logo_path1 = "./img/pnnl-logo.png"
logo_path2 = "./img/gdo-logo.png"

try:
    with logo_col1:
        st.image(logo_path1, width=200) # Adjust width as needed
except FileNotFoundError:
    with logo_col1:
        st.warning(f"PNNL logo not found at {logo_path1}", icon="üñºÔ∏è")
except Exception as e:
     with logo_col1:
        st.error(f"Error loading PNNL logo: {e}", icon="üö®")


try:
    with logo_col2:
        st.image(logo_path2, width=200) # Adjust width as needed
except FileNotFoundError:
    with logo_col2:
        st.warning(f"GDO logo not found at {logo_path2}", icon="üñºÔ∏è")
except Exception as e:
     with logo_col2:
        st.error(f"Error loading GDO logo: {e}", icon="üö®")

# --- Remaining Imports ---
import fitz  # PyMuPDF
import faiss
import os
import hashlib
import logging # Import logging
# Updated OpenAI/Azure imports
from langchain_openai import OpenAIEmbeddings, ChatOpenAI, AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS # Using community import
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field # For function calling output schema
# Import MultiQueryRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from dotenv import load_dotenv
# Updated typing import to include Optional
from typing import List, TypedDict, Annotated, Sequence, Dict, Optional, Tuple
import operator
# Import for parallel execution
import concurrent.futures
# Import for graph visualization
from io import BytesIO
import warnings

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage

# --- Logging Setup ---
# Optional: Configure logging to see the queries generated by MultiQueryRetriever
logging.basicConfig(level=logging.INFO) # Set default level
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)


# --- Azure OpenAI Configuration ---
# Load environment variables (especially Azure credentials)
load_dotenv()

api_key = os.getenv("AZURE_OPENAI_API_KEY", default=None)
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", default=None)
# Use a default deployment name if not set, or handle error if critical
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o") # Defaulting to gpt-4o deployment name
embeddings_deployment = os.getenv("AZURE_EMBEDDINGS_DEPLOYMENT", "text-embedding-3-large") # Defaulting embeddings deployment
openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01") # Defaulting API version

# --- Check for Azure Credentials (AFTER set_page_config) ---
azure_creds_valid = True
if not api_key or not azure_endpoint:
    st.error("Azure OpenAI API key or endpoint not found. Please set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT environment variables.", icon="üö®")
    azure_creds_valid = False
    # Consider stopping execution if credentials are required for core functionality
    # st.stop() # Uncomment if the app cannot proceed without credentials

# Constants for predefined questions
PREDEFINED_QUESTIONS = [
    "Enter my own question...",
    "Does the plan include vegetation management? If so, how much money is allocated to it?",
    "Does the plan include undergrounding? If so, how much money is allocated to it?",
    "Does the plan include PSPS? If so, how much money is allocated to it?",
    "How frequently does the utility perform asset inspections?",
    "Are there generation considerations, such as derating solar PV during smoky conditions?"
]

# --- Helper Functions (PDF Processing - Unchanged) ---

def extract_pages_from_pdf(pdf_file):
    """Extracts text page by page from an uploaded PDF file, returning a list of (page_number, text)."""
    pages_content = []
    try:
        # Use file content directly if already read, otherwise read from UploadedFile
        if isinstance(pdf_file, bytes):
             pdf_bytes = pdf_file
        else:
             pdf_bytes = pdf_file.getvalue() # For streamlit UploadedFile

        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text()
            if text:
                pages_content.append((page_num + 1, text))
        doc.close()
        return pages_content
    except Exception as e:
        # Use st.error only if in streamlit context, otherwise print
        try:
            st.error(f"Error reading PDF: {e}", icon="üìÑ")
        except Exception:
             print(f"Error reading PDF: {e}")
        return None

# --- Caching (Retriever Creation - Returns BASE retriever) ---
# Modified to take file content bytes directly for better caching with multiple files
@st.cache_resource(show_spinner="Processing PDF: {_file_name}") # Use _file_name in spinner
def get_base_retriever_from_pdf_bytes(_file_content_bytes, _file_name="Unknown"):
    """Processes PDF bytes, creates Azure embeddings, builds FAISS store, returns BASE retriever."""
    if not api_key or not azure_endpoint:
        # Use st.error only if in streamlit context
        try:
            st.error("Azure credentials not available for PDF processing.", icon="üö®")
        except Exception:
            print("Error: Azure credentials not available for PDF processing.")
        return None

    try:
        # st.write(f"Processing file: {_file_name}") # Now shown in spinner
        pages_data = extract_pages_from_pdf(_file_content_bytes) # Pass bytes directly
        if not pages_data:
            # Use st.warning only if in streamlit context
            try:
                st.warning(f"Could not extract text from the PDF: {_file_name}", icon="‚ö†Ô∏è")
            except Exception:
                 print(f"Warning: Could not extract text from the PDF: {_file_name}")
            return None

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200, length_function=len
        )
        all_docs = []
        for page_num, page_text in pages_data:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                # Add filename to metadata
                doc = Document(page_content=chunk, metadata={"page": page_num, "source": _file_name})
                all_docs.append(doc)

        if not all_docs:
             try:
                st.warning(f"Could not create any text chunks from the PDF: {_file_name}", icon="‚ö†Ô∏è")
             except Exception:
                  print(f"Warning: Could not create any text chunks from the PDF: {_file_name}")
             return None
        # print(f"Split PDF '{_file_name}' into {len(all_docs)} chunks.") # Use print for background

        # --- Instantiate Azure Embeddings ---
        # print(f"Using Azure embeddings deployment: {embeddings_deployment}") # Use print
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=embeddings_deployment,
            azure_endpoint=azure_endpoint,
            api_key=api_key,
            openai_api_version=openai_api_version
        )

        # print(f"Building FAISS index for '{_file_name}'...") # Use print
        vectorstore = FAISS.from_documents(documents=all_docs, embedding=embeddings)
        # print(f"FAISS index built successfully for '{_file_name}'.") # Use print
        # Return the base retriever
        return vectorstore.as_retriever(search_kwargs={'k': 10})
    except Exception as e:
        try:
            st.error(f"Error processing PDF '{_file_name}': {e}", icon="‚ùå")
            st.exception(e)
        except Exception:
             print(f"Error processing PDF '{_file_name}': {e}")
        return None

# --- LangGraph State Definition (For Individual File Analysis) ---

class IndividualFileGraphState(TypedDict):
    """
    Represents the state for analyzing a single file.

    Attributes:
        question: The user's question.
        generation: LLM generation for this specific file.
        documents: list of documents from this file (potentially filtered).
        original_documents: list of documents retrieved initially for this file.
        base_retriever: The base FAISS retriever object for this specific file.
        file_name: The name of the PDF file being processed.
    """
    question: str
    generation: Optional[str] # Can be None if irrelevant
    documents: List[Document]
    original_documents: List[Document]
    base_retriever: object
    file_name: str # Added filename to state

# --- LangGraph Nodes (Modified for Individual File Analysis - Removed st.write) ---

def retrieve_docs_multi_query(state: IndividualFileGraphState) -> IndividualFileGraphState:
    """ Uses MultiQueryRetriever for a single file. """
    file_name = state["file_name"]
    print(f"--- Retrieving Documents for '{file_name}' using Multi-Query ---") # Use print for logs
    question = state["question"]
    base_retriever = state["base_retriever"]

    if not base_retriever:
        print(f"Error: Base retriever not available for {file_name}.") # Use print
        return {"original_documents": [], "question": question, "file_name": file_name}

    if not azure_creds_valid:
         print(f"Warning: Skipping multi-query for '{file_name}' due to missing Azure credentials.") # Use print
         try:
             print(f"Falling back to simple retrieval for '{file_name}'...") # Use print
             documents = base_retriever.get_relevant_documents(question)
             print(f"Retrieved {len(documents)} documents via simple fallback for '{file_name}'.") # Use print
             return {"original_documents": documents, "question": question, "file_name": file_name}
         except Exception as e:
             print(f"Error during simple fallback retrieval for '{file_name}': {e}") # Use print
             return {"original_documents": [], "question": question, "file_name": file_name}

    try:
        llm_for_queries = AzureChatOpenAI(
            temperature=0, api_key=api_key, openai_api_version=openai_api_version,
            azure_deployment=deployment, azure_endpoint=azure_endpoint,
        )
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever, llm=llm_for_queries
        )
        unique_docs = multi_query_retriever.get_relevant_documents(query=question)
        print(f"Retrieved {len(unique_docs)} unique documents via Multi-Query for '{file_name}'.") # Use print
        return {"original_documents": unique_docs, "question": question, "file_name": file_name}
    except Exception as e:
        print(f"Error during multi-query retrieval for '{file_name}': {e}") # Use print
        return {"original_documents": [], "question": question, "file_name": file_name}

# --- Filter Documents Node ---
class FilteredDocs(BaseModel):
    keep_indices: List[int] = Field(description="List of zero-based indices of the documents that should be kept (i.e., are not structural or irrelevant).")

def filter_documents(state: IndividualFileGraphState) -> IndividualFileGraphState:
    """ Filters documents for a single file. """
    file_name = state["file_name"]
    print(f"--- Filtering Structurally Irrelevant Documents for '{file_name}' ---") # Use print
    original_documents = state["original_documents"]
    question = state["question"]

    if not original_documents:
        print(f"No documents to filter for '{file_name}'.") # Use print
        return {"documents": [], **state}

    if not azure_creds_valid:
         print(f"Warning: Skipping filtering for '{file_name}' due to missing Azure credentials.") # Use print
         return {"documents": original_documents, **state}

    try:
        filtering_llm = AzureChatOpenAI(
            temperature=0, api_key=api_key, openai_api_version=openai_api_version,
            azure_deployment=deployment, azure_endpoint=azure_endpoint,
        ).bind_tools([FilteredDocs], tool_choice="FilteredDocs")

        doc_context = ""
        for i, doc in enumerate(original_documents):
            source_name = doc.metadata.get('source', file_name)
            doc_context += f"--- Document Index {i} (Source: {source_name}, Page {doc.metadata.get('page', 'N/A')}) ---\n{doc.page_content}\n\n"

        filtering_prompt = PromptTemplate(
            template="""You are an expert document analyst. Your task is to identify document chunks that are primarily structural elements like Table of Contents entries, page headers/footers, indices, or reference lists, which are unlikely to contain direct answers to content-based questions about a technical report. Analyze the following document chunks provided below, each marked with a 'Document Index' and 'Source'. Based on the content of each chunk, identify the indices of the documents that ARE LIKELY TO CONTAIN SUBSTANTIVE CONTENT (prose, data, analysis, findings) and should be kept for further analysis. Ignore chunks that are just lists of section titles with page numbers (like a ToC), repetitive headers/footers, or bibliographies unless they uniquely contain relevant information not present elsewhere. Use the 'FilteredDocs' tool to return the list of indices to keep. Return ONLY the indices to keep. Document Chunks:\n{doc_context}""",
            input_variables=["doc_context"],
        )

        filtering_chain = filtering_prompt | filtering_llm
        response = filtering_chain.invoke({"doc_context": doc_context})

        kept_indices = []
        if response.tool_calls and response.tool_calls[0]['name'] == 'FilteredDocs':
            kept_indices = response.tool_calls[0]['args'].get('keep_indices', [])
            print(f"LLM identified {len(kept_indices)} documents to keep for '{file_name}'.") # Use print
        else:
             print(f"Warning: Filtering LLM did not return expected format for '{file_name}'. Keeping all.") # Use print
             return {"documents": original_documents, **state}

        filtered_docs = [original_documents[i] for i in kept_indices if i < len(original_documents)]
        print(f"Filtered down to {len(filtered_docs)} documents for '{file_name}'.") # Use print
        return {"documents": filtered_docs, **{k: v for k, v in state.items() if k != 'documents'}}

    except Exception as e:
        print(f"Error during document filtering for '{file_name}': {e}") # Use print
        print(f"Warning: Filtering failed for '{file_name}'. Passing all original documents to grading.") # Use print
        return {"documents": original_documents, **state}


# --- Grade Documents Node ---
def grade_documents(state: IndividualFileGraphState) -> IndividualFileGraphState:
    """ Grades documents for a single file. """
    file_name = state["file_name"]
    print(f"--- Grading Filtered Document Relevance for '{file_name}' ---") # Use print
    question = state["question"]
    documents = state["documents"]

    if not documents:
         print(f"No documents remaining after filtering to grade for '{file_name}'.") # Use print
         return {"documents": [], **state}

    if not azure_creds_valid:
         print(f"Warning: Skipping grading for '{file_name}' due to missing Azure credentials.") # Use print
         return {"documents": documents, **state} # Assume relevant

    try:
        llm = AzureChatOpenAI(
            temperature=0, api_key=api_key, openai_api_version=openai_api_version,
            azure_deployment=deployment, azure_endpoint=azure_endpoint,
        )
        grading_function = {
            "name": "grade_relevance", "description": "Determine if the provided document chunks are relevant to the user's question.",
            "parameters": { "type": "object", "properties": { "relevant": { "type": "string", "enum": ["yes", "no"], "description": "Whether the documents contain information relevant to answering the question."}}, "required": ["relevant"]},
        }
        prompt = PromptTemplate(
            template="""You are a grader assessing relevance of potentially relevant document chunks to a user question. These chunks have already been pre-filtered to remove obvious structural elements. Analyze the following document chunks based on the user question:\n-------\nUser Question: {question}\n-------\nDocument Chunks:\n{documents}\n-------\nDetermine if these remaining chunks contain information that can directly answer the user's question. Respond using the 'grade_relevance' function call. Ensure your response strictly adheres to the function call format.""",
            input_variables=["question", "documents"],
        )
        doc_texts = "\n\n".join([d.page_content for d in documents])
        grader_chain = prompt | llm.bind_tools([grading_function], tool_choice="grade_relevance")
        response = grader_chain.invoke({"question": question, "documents": doc_texts})

        is_relevant = "no"
        if response.tool_calls and response.tool_calls[0]['name'] == 'grade_relevance':
            is_relevant = response.tool_calls[0]['args'].get('relevant', 'no')
            print(f"Relevance Grade for '{file_name}': {is_relevant.upper()}") # Use print
        else:
             print(f"Warning: Grader LLM did not return expected format for '{file_name}'.") # Use print
             is_relevant = "no"

        if is_relevant == "yes":
            print(f"Decision for '{file_name}': Documents are relevant.") # Use print
            return {**state}
        else:
            print(f"Decision for '{file_name}': Documents not relevant.") # Use print
            return {"documents": [], **{k: v for k, v in state.items() if k != 'documents'}}

    except Exception as e:
        print(f"Error during document grading for '{file_name}': {e}") # Use print
        return {"documents": [], **state}


# --- Generate Answer Node ---
def generate_answer(state: IndividualFileGraphState) -> IndividualFileGraphState:
    """ Generates answer for a single file. """
    file_name = state["file_name"]
    print(f"--- Generating Answer for '{file_name}' ---") # Use print
    question = state["question"]
    documents = state["documents"]

    if not documents:
        print(f"Warning: Generation called with no relevant documents for '{file_name}'.") # Use print
        return {"generation": None, **state}

    if not azure_creds_valid:
         print(f"Warning: Skipping generation for '{file_name}' due to missing Azure credentials.") # Use print
         return {"generation": f"Could not generate answer for {file_name} due to missing credentials.", **state}

    try:
        context = "\n\n".join([f"--- Context from File '{d.metadata.get('source', file_name)}' Page {d.metadata.get('page', 'N/A')} ---\n{d.page_content}" for d in documents])

        prompt_template = """You are an expert assistant analyzing a specific technical report ({file_name}). Your task is to answer the user's question comprehensively based *only* on the provided context chunks from THIS FILE. Follow these instructions carefully: 1. Thoroughly read all provided context chunks for this file, each marked with '--- Context from File {file_name} Page X ---'. Pay attention to the page numbers. 2. Answer the user's question directly based *only* on the information present in the context from THIS FILE ({file_name}). 3. Identify the most relevant information or statement(s) within the context that directly address the question. 4. **Contextualized Quoting:** When presenting this key information, include a direct quote (enclosed in double quotation marks) of the most relevant sentence or phrase. **Crucially, also explain the surrounding context from the *same chunk* to clarify the quote's meaning or provide necessary background.** Cite the page number in parentheses after the quote, like this: `"quote..." (Page X)`. 5. If the question asks about specific details (like financial allocations, frequencies, specific procedures) and that detail is *not* found in the context from THIS FILE, explicitly state that the information is not provided in this specific document ({file_name}). Do not make assumptions or provide external knowledge from other documents. 6. **Structure for Clarity:** Structure your answer logically. Start with a direct summary answer based on this file. Then, present the supporting details using the contextualized quoting method described above. Conclude by addressing any parts of the question that couldn't be answered from this file's context. If no relevant information is found in the provided context to answer the question, state that clearly for this file ({file_name}). Context from Document '{file_name}':\n{context}\n\nQuestion: {question}\n\nAnswer based ONLY on '{file_name}':"""
        prompt = PromptTemplate(template=prompt_template, input_variables=["file_name", "context", "question"])

        llm = AzureChatOpenAI(
            temperature=0.1, api_key=api_key, openai_api_version=openai_api_version,
            azure_deployment=deployment, azure_endpoint=azure_endpoint,
        )

        rag_chain = prompt | llm | StrOutputParser()
        generation = rag_chain.invoke({"context": context, "question": question, "file_name": file_name})
        print(f"Answer generated for '{file_name}'.") # Use print
        return {"generation": generation, **{k: v for k, v in state.items() if k != 'generation'}}

    except Exception as e:
        print(f"Error during answer generation for '{file_name}': {e}") # Use print
        return {"generation": f"An error occurred while generating the answer for {file_name}: {e}", **state}


# --- LangGraph Conditional Edge (Unchanged Logic) ---
def decide_to_generate(state: IndividualFileGraphState) -> str:
    """ Determines whether to generate an answer for the current file. """
    documents = state["documents"]
    if not documents:
        return "end_no_relevance"
    else:
        return "generate"

# --- Build LangGraph for Individual File Analysis ---

# Define the workflow graph using the individual file state
workflow = StateGraph(IndividualFileGraphState)

# Define the nodes
workflow.add_node("retrieve", retrieve_docs_multi_query)
workflow.add_node("filter_documents", filter_documents)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate_answer)

# Build graph
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "filter_documents")
workflow.add_edge("filter_documents", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "generate": "generate",
        "end_no_relevance": END # End this branch if no relevant docs
    }
)
workflow.add_edge("generate", END) # End branch after generation

# Compile the graph only if Azure credentials are valid
individual_file_analyzer_app = None
graph_visualization = None # Variable to store graph image bytes
if azure_creds_valid:
    try:
        individual_file_analyzer_app = workflow.compile()
        # Attempt to generate graph visualization (requires pygraphviz and Graphviz)
        try:
            # Filter UserWarning from Pydantic related to model_rebuild
            # This warning often appears with LangGraph visualization and can be ignored
            warnings.filterwarnings("ignore", message=".*`model_rebuild`.*")
            # Generate the graph image bytes
            graph_visualization = individual_file_analyzer_app.get_graph().draw_mermaid_png()
            warnings.resetwarnings() # Reset warnings filter after generation
            print("Successfully generated graph visualization.") # Log success
        except ImportError:
            print("Info: pygraphviz not installed, skipping graph visualization.")
        except Exception as viz_err:
            # Catch broader errors during visualization generation
            print(f"Warning: Failed to generate graph visualization: {viz_err}")
            # Optionally log traceback for debugging viz errors
            # import traceback
            # print(traceback.format_exc())

    except Exception as e:
        st.error(f"Failed to compile individual file analysis graph: {e}")
        st.exception(e)

# --- Consolidation Function ---
def consolidate_answers(question: str, individual_results: List[Dict[str, str]]) -> Optional[str]:
    """
    Uses an LLM to consolidate answers from multiple documents into a single response.

    Args:
        question: The original user question.
        individual_results: A list of dictionaries, each containing 'file_name' and 'answer'.

    Returns:
        A single consolidated answer string, or None if consolidation fails.
    """
    print("--- Consolidating Answers from Multiple Files ---") # Use print
    if not individual_results:
        print("Warning: No individual answers provided for consolidation.") # Use print
        return "No relevant information found in the uploaded documents."

    if not azure_creds_valid:
        print("Error: Cannot consolidate answers due to missing Azure credentials.") # Use print
        return "Consolidation failed due to missing credentials."

    # Prepare the combined context from individual answers
    combined_context = ""
    valid_results_count = 0
    for result in individual_results:
        answer = result.get('answer')
        # Filter out non-answers more robustly
        if answer and "No relevant information found" not in answer and "An error occurred" not in answer and "Could not generate answer" not in answer:
             combined_context += f"--- Analysis from File: {result['file_name']} ---\n"
             combined_context += f"{answer}\n\n"
             valid_results_count += 1

    if not combined_context:
         print("Warning: None of the individual analyses produced relevant answers for consolidation.") # Use print
         return "Although the documents were processed, no specific relevant information matching the question was found in any of them."
    else:
         print(f"Consolidating {valid_results_count} valid individual answers.") # Use print


    consolidation_prompt_template = """You are an expert synthesis agent. Your task is to consolidate findings from analyses of multiple technical reports into a single, comprehensive answer to the user's original question. You have been provided with the original question and a series of answers, each derived from analyzing a specific PDF file. Each individual answer already contains contextualized quotes and page number citations relative to its source file. Your goal is to: 1. Understand the original user question. 2. Review all the individual answers provided below, noting which file each answer came from. 3. Synthesize these findings into a single, coherent response that directly addresses the original question. 4. Combine similar findings from different documents where appropriate. Highlight any agreements or discrepancies between the documents if significant. 5. **Crucially: Preserve Source Attribution.** When incorporating specific details or quotes from the individual answers, you MUST cite both the original **file name** and the **page number** as provided in the individual answers. The expected citation format is `(Source: file_name.pdf, Page X)`. Adapt the citations found in the individual answers (which might just be `(Page X)`) to include the filename. 6. Structure the final answer logically. Start with an overall summary if possible, then elaborate with synthesized details and properly attributed quotes/information from the source documents. 7. If some documents provided relevant information while others didn't, mention this clearly in the consolidated response (e.g., "Document X did not contain specific details on this topic."). Original User Question: {question} Individual Answers from Documents:\n{combined_context}\n\nConsolidated Answer (with citations like '(Source: file_name.pdf, Page X)'):"""
    consolidation_prompt = ChatPromptTemplate.from_template(consolidation_prompt_template)

    try:
        # print(f"Using Azure chat deployment for consolidation: {deployment}") # Use print
        consolidation_llm = AzureChatOpenAI(
            temperature=0.2, # Slightly higher temp for synthesis
            api_key=api_key,
            openai_api_version=openai_api_version,
            azure_deployment=deployment,
            azure_endpoint=azure_endpoint,
            max_tokens=2000 # Increase max_tokens for potentially longer consolidated answers
        )

        consolidation_chain = consolidation_prompt | consolidation_llm | StrOutputParser()
        consolidated_answer = consolidation_chain.invoke({
            "question": question,
            "combined_context": combined_context
        })
        print("Consolidation complete.") # Use print
        return consolidated_answer

    except Exception as e:
        print(f"Error during answer consolidation: {e}") # Use print
        return f"An error occurred during the consolidation step: {e}"

# --- Function to run analysis for one file (for parallel execution) ---
def analyze_single_file(args: Tuple[str, str, object, str]) -> Dict[str, str]:
    """
    Wrapper function to run the LangGraph analysis for a single file.
    Takes arguments as a tuple for compatibility with executor.map.
    """
    file_id, question, base_retriever, file_name = args
    print(f"Starting analysis thread for: {file_name} (ID: {file_id})")
    if not individual_file_analyzer_app:
        print(f"Error: Analysis graph not compiled. Cannot analyze {file_name}.")
        return {"file_name": file_name, "answer": "Error: Analysis graph not compiled."}
    try:
        initial_state = {
            "question": question,
            "base_retriever": base_retriever,
            "file_name": file_name
        }
        # Invoke the compiled graph
        final_state = individual_file_analyzer_app.invoke(initial_state)
        individual_answer = final_state.get("generation")
        print(f"Finished analysis thread for: {file_name}")
        return {
            "file_name": file_name,
            "answer": individual_answer if individual_answer else "No relevant information found in this document."
        }
    except Exception as e:
        print(f"Error analyzing file {file_name} in thread: {e}")
        # Optionally log the full traceback here
        # import traceback
        # print(traceback.format_exc())
        return {
            "file_name": file_name,
            "answer": f"An error occurred during analysis: {e}"
        }


# --- Streamlit UI ---
components.html(
    '<div><img src="./img/wildfire-logo.png" width="50" style="vertical-align:middle; margin-right:10px;"/> Wildfire Mitigation Plans Database - AI Assistant</div>',
    height=60
)

st.markdown("""
Upload **multiple** technical reports (PDFs). The system will analyze each PDF individually
for your question **in parallel** and then consolidate the findings into a single answer, citing the source file and page for quotes.
""")

# --- File Upload ---
st.divider() # Add a divider after title/logos
if azure_creds_valid:
    # Allow multiple files
    uploaded_files = st.file_uploader(
        "Choose PDF files",
        type="pdf",
        accept_multiple_files=True # Key change here
    )
else:
    uploaded_files = [] # Ensure it's an empty list if creds invalid
    st.warning("File uploader disabled until Azure credentials are provided.")


# --- Process Uploaded Files & Store Retrievers ---
# Store retrievers in session state to avoid reprocessing
if "retrievers" not in st.session_state:
    st.session_state.retrievers = {} # Dict to store {file_id: retriever}
if "processed_hashes" not in st.session_state:
    st.session_state.processed_hashes = set() # Keep track of processed file hashes
if "processed_files_info" not in st.session_state:
     st.session_state.processed_files_info = [] # Store info {id, name}

# Logic to update processed files based on current upload
current_file_ids = set()
files_to_process = []
if azure_creds_valid and uploaded_files:
     # Identify current file IDs and files needing processing
     for uploaded_file in uploaded_files:
          file_content_bytes = uploaded_file.getvalue() # Read bytes once
          file_hash = hashlib.md5(file_content_bytes).hexdigest()
          file_id = f"{uploaded_file.name}_{file_hash[:8]}"
          current_file_ids.add(file_id)
          if file_hash not in st.session_state.processed_hashes:
               files_to_process.append({
                    "file": uploaded_file,
                    "bytes": file_content_bytes,
                    "hash": file_hash,
                    "id": file_id,
                    "name": uploaded_file.name
               })
          elif file_id not in st.session_state.retrievers:
               # Hash known but retriever missing (e.g., after cache clear/restart) - mark for reprocessing
                files_to_process.append({
                    "file": uploaded_file,
                    "bytes": file_content_bytes,
                    "hash": file_hash,
                    "id": file_id,
                    "name": uploaded_file.name
               })


     # Remove info for files that are no longer uploaded
     st.session_state.processed_files_info = [
          info for info in st.session_state.processed_files_info if info["id"] in current_file_ids
     ]
     # Remove retrievers for files no longer uploaded
     retrievers_to_remove = [fid for fid in st.session_state.retrievers if fid not in current_file_ids]
     for fid in retrievers_to_remove:
          del st.session_state.retrievers[fid]
          # Find corresponding hash to remove (less efficient, but necessary if only ID is known here)
          hash_to_remove = None
          if "file_id_to_hash_map" in st.session_state: # Check if map exists
                # Invert map for easier lookup (or store both ways)
                hash_to_id_map = {v: k for k, v in st.session_state.file_id_to_hash_map.items()}
                hash_to_remove = hash_to_id_map.get(fid)

          if hash_to_remove and hash_to_remove in st.session_state.processed_hashes:
               st.session_state.processed_hashes.remove(hash_to_remove)
          if "file_id_to_hash_map" in st.session_state and hash_to_remove in st.session_state.file_id_to_hash_map:
                del st.session_state.file_id_to_hash_map[hash_to_remove] # Remove from map


     # Process new or missing files
     if files_to_process:
          # Use st.status for the processing spinner
          with st.status(f"Processing {len(files_to_process)} PDF file(s)...", expanded=False) as process_status:
               if "file_id_to_hash_map" not in st.session_state: st.session_state.file_id_to_hash_map = {} # Init map

               for file_info in files_to_process:
                    process_status.update(label=f"Processing: {file_info['name']}")
                    try:
                         # Call cached function with bytes and name
                         retriever = get_base_retriever_from_pdf_bytes(file_info["bytes"], _file_name=file_info["name"])
                         if retriever:
                              st.session_state.retrievers[file_info["id"]] = retriever
                              st.session_state.processed_hashes.add(file_info["hash"])
                              st.session_state.file_id_to_hash_map[file_info["hash"]] = file_info["id"] # Store mapping
                              # Add to processed_files_info only if not already there
                              if not any(p_info["id"] == file_info["id"] for p_info in st.session_state.processed_files_info):
                                   st.session_state.processed_files_info.append({"id": file_info["id"], "name": file_info["name"]})
                              # st.success(f"Processed and cached retriever for: {file_info['name']}", icon="‚úÖ") # Too verbose inside spinner
                         else:
                              st.warning(f"Failed to process: {file_info['name']}", icon="‚ö†Ô∏è")
                    except Exception as e:
                         st.error(f"Error processing file {file_info['name']}: {e}")
               process_status.update(label="PDF Processing Complete.", state="complete", expanded=False)

# If no files are uploaded now, clear everything
if not uploaded_files:
    st.session_state.retrievers = {}
    st.session_state.processed_hashes = set()
    st.session_state.processed_files_info = []
    if "file_id_to_hash_map" in st.session_state: del st.session_state.file_id_to_hash_map


# Display names of currently loaded files
if st.session_state.processed_files_info:
    st.markdown("**Currently Loaded Documents:**")
    # Use columns for a potentially cleaner list display if many files
    cols = st.columns(3) # Adjust number of columns as needed
    col_idx = 0
    for info in st.session_state.processed_files_info:
        with cols[col_idx % len(cols)]:
             st.markdown(f"- {info['name']}")
        col_idx += 1
    # Add empty markdown to ensure columns align if list isn't full multiple
    while col_idx % len(cols) != 0:
        with cols[col_idx % len(cols)]:
            st.markdown("") # Empty placeholder
        col_idx += 1

else:
    st.info("Upload one or more PDF files to begin.")


# --- Question Input ---
st.divider()
st.subheader("Ask a Question Across All Uploaded Documents")

# Disable input if no files processed or creds invalid
input_disabled = not st.session_state.retrievers or not azure_creds_valid

col1, col2 = st.columns([3, 1])

with col1:
    selected_question = st.selectbox(
        "Select a predefined question or choose 'Enter my own question...':",
        PREDEFINED_QUESTIONS,
        index=0,
        key="question_select",
        disabled=input_disabled
    )
    custom_question_disabled = selected_question != PREDEFINED_QUESTIONS[0] or input_disabled
    custom_question = st.text_input(
        "Enter your custom question here:",
        key="custom_question_input",
        disabled=custom_question_disabled,
        placeholder="Type your question..." if not input_disabled else "Upload valid PDFs first"
    )

with col2:
    st.markdown("<br/>", unsafe_allow_html=True)
    # Disable button if no retrievers, no compiled graph, or creds missing
    ask_button_disabled = not st.session_state.retrievers or not individual_file_analyzer_app or not azure_creds_valid
    ask_button = st.button("Get Consolidated Answer", type="primary", use_container_width=True, disabled=ask_button_disabled)

if selected_question == PREDEFINED_QUESTIONS[0]:
    final_question = custom_question
else:
    final_question = selected_question

# --- Answer Generation and Display (Parallel Loop, Consolidate) ---
st.divider()
st.subheader("Consolidated Answer")

answer_placeholder = st.empty()
individual_results_placeholder = st.empty() # To show individual results optionally

# Only run if button clicked AND prerequisites met
if ask_button and st.session_state.retrievers and final_question and individual_file_analyzer_app and azure_creds_valid:
    answer_placeholder.info("Analyzing documents in parallel and consolidating...", icon="‚è≥")
    individual_results_placeholder.empty() # Clear previous individual results

    all_individual_results = []
    tasks = []
    # Prepare arguments for parallel execution
    for file_id, base_retriever in st.session_state.retrievers.items():
        current_file_name = "Unknown File" # Find the actual name
        for info in st.session_state.processed_files_info:
            if info["id"] == file_id:
                current_file_name = info["name"]
                break
        tasks.append((file_id, final_question, base_retriever, current_file_name))

    # Use ThreadPoolExecutor for parallel execution
    num_files = len(tasks)
    # Use st.status for better progress indication during the loop
    with st.status(f"Analyzing {num_files} document(s) in parallel...", expanded=True) as status:
        all_individual_results = []
        # Use max_workers to limit concurrency if needed, defaults to number of cores * 5
        # Set a reasonable default like 4 to avoid overwhelming resources/APIs
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            # Use executor.map or submit/as_completed
            # submit/as_completed allows better progress tracking
            future_to_task = {executor.submit(analyze_single_file, task): task for task in tasks}
            results_processed = 0
            for future in concurrent.futures.as_completed(future_to_task):
                task_info = future_to_task[future]
                file_name_for_status = task_info[3] # Get filename from task tuple
                try:
                    result = future.result() # Get result from completed future
                    all_individual_results.append(result)
                    results_processed += 1
                    status.update(label=f"Analyzed {results_processed}/{num_files}: {file_name_for_status} complete.")
                except Exception as exc:
                    # Log error from thread
                    print(f"Error analyzing file {file_name_for_status} in parallel thread: {exc}")
                    st.error(f"Error analyzing file {file_name_for_status} in parallel: {exc}") # Show error in UI too
                    all_individual_results.append({
                         "file_name": file_name_for_status,
                         "answer": f"An error occurred during analysis: {exc}"
                    })
                    results_processed += 1 # Count errors as processed
                    status.update(label=f"Error analyzing {file_name_for_status}. Processed {results_processed}/{num_files}.", state="running")


        status.update(label="Analysis complete. Consolidating results...", state="running")
        # Consolidate the answers (ensure results are in a somewhat consistent order if needed)
        all_individual_results.sort(key=lambda x: x['file_name']) # Sort by filename for consistency
        consolidated_answer = consolidate_answers(final_question, all_individual_results)
        status.update(label="Consolidation complete!", state="complete", expanded=False)


    # Display the final consolidated answer
    if consolidated_answer:
         # Use markdown for better formatting control if needed
         answer_placeholder.markdown(consolidated_answer)
         # Or keep using success if preferred:
         # answer_placeholder.success(consolidated_answer, icon="‚úÖ")
    else:
        # Handle case where consolidation itself failed or produced no answer
        answer_placeholder.error("Failed to generate consolidated answer or no relevant info found.", icon="‚ùå")

    # Optionally display individual results in an expander
    with individual_results_placeholder.expander("Show Individual Document Analyses"):
        if all_individual_results:
            for result in all_individual_results:
                st.markdown(f"**Analysis for: {result['file_name']}**")
                st.markdown(result['answer'] if result.get('answer') else "Analysis did not produce an answer.")
                st.divider()
        else:
            st.write("No individual analysis results were generated.")


# Handle other button click conditions
elif ask_button and not st.session_state.retrievers:
    answer_placeholder.warning("Please upload and process at least one PDF file first.", icon="‚ö†Ô∏è")
elif ask_button and not final_question:
     answer_placeholder.warning("Please enter or select a question.", icon="‚ö†Ô∏è")
elif ask_button and (not individual_file_analyzer_app or not azure_creds_valid):
     if not azure_creds_valid:
         answer_placeholder.error("Cannot get answer: Azure credentials missing.", icon="üö®")
     elif not individual_file_analyzer_app:
         answer_placeholder.error("Cannot get answer: Analysis graph failed to compile.", icon="üö®")
# Initial state message
else:
    if azure_creds_valid:
        # Display message based on whether files are loaded
        if not st.session_state.retrievers:
             answer_placeholder.info("Upload one or more PDF files and ask a question to get started.")
        # else: # If files are loaded but no question asked yet, no message needed here
    # Missing creds message handled by initial check


# --- Footer ---
st.divider()
# Display graph visualization if available
if graph_visualization:
    with st.expander("Show Workflow Graph (Individual File Analysis)"):
        st.image(graph_visualization)
else:
    st.caption("Graph visualization skipped (requires pygraphviz/Graphviz).")

st.caption("Powered by LangChain, LangGraph, Azure OpenAI, FAISS, and Streamlit")

